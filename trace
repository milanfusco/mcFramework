versions pytest-8.2.0, python-3.12.3.final.0
invocation_dir=/Users/milanfusco/PycharmProjects/McFramework
cwd=/Users/milanfusco/PycharmProjects/McFramework
args=('--cov=src/mcframework', 'tests/', '--debug=trace')

  pytest_cmdline_main [hook]
      config: <_pytest.config.Config object at 0x100df67b0>
    pytest_plugin_registered [hook]
        plugin: <Session  exitstatus='<UNSET>' testsfailed=0 testscollected=0>
        plugin_name: session
        manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
    finish pytest_plugin_registered --> [] [hook]
    pytest_configure [hook]
        config: <_pytest.config.Config object at 0x100df67b0>
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x1073ec440>
          plugin_name: lfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x1052f5dc0>
          plugin_name: nfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: faulthandler [assertion]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          plugin_name: legacypath-tmpdir
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: pdb [assertion]
    early skip of rewriting module: cmd [assertion]
    early skip of rewriting module: code [assertion]
    early skip of rewriting module: codeop [assertion]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x1011faa80>
          plugin_name: 4313819776
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x100df67b0>
          plugin_name: pytestconfig
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/mark/__init__.py'>
          plugin_name: mark
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/main.py'>
          plugin_name: main
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/runner.py'>
          plugin_name: runner
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/fixtures.py'>
          plugin_name: fixtures
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/helpconfig.py'>
          plugin_name: helpconfig
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/python.py'>
          plugin_name: python
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/terminal.py'>
          plugin_name: terminal
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/debugging.py'>
          plugin_name: debugging
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/unittest.py'>
          plugin_name: unittest
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/capture.py'>
          plugin_name: capture
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/skipping.py'>
          plugin_name: skipping
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/legacypath.py'>
          plugin_name: legacypath
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/tmpdir.py'>
          plugin_name: tmpdir
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py'>
          plugin_name: monkeypatch
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/recwarn.py'>
          plugin_name: recwarn
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/pastebin.py'>
          plugin_name: pastebin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/__init__.py'>
          plugin_name: assertion
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/junitxml.py'>
          plugin_name: junitxml
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/doctest.py'>
          plugin_name: doctest
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/cacheprovider.py'>
          plugin_name: cacheprovider
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/freeze_support.py'>
          plugin_name: freeze_support
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/setuponly.py'>
          plugin_name: setuponly
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/setupplan.py'>
          plugin_name: setupplan
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/stepwise.py'>
          plugin_name: stepwise
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/warnings.py'>
          plugin_name: warnings
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/logging.py'>
          plugin_name: logging
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/reports.py'>
          plugin_name: reports
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/python_path.py'>
          plugin_name: python_path
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/unraisableexception.py'>
          plugin_name: unraisableexception
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/threadexception.py'>
          plugin_name: threadexception
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/faulthandler.py'>
          plugin_name: faulthandler
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_cov.plugin' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest_cov/plugin.py'>
          plugin_name: pytest_cov
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          plugin_name: capturemanager
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <pytest_cov.plugin.CovPlugin object at 0x101628140>
          plugin_name: _cov
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'conftest' from '/Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py'>
          plugin_name: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          plugin_name: session
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x1073ec440>
          plugin_name: lfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x1052f5dc0>
          plugin_name: nfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          plugin_name: legacypath-tmpdir
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x1073edeb0>
          plugin_name: terminalreporter
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x1073eda30>
          plugin_name: logging-plugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
    finish pytest_configure --> [] [hook]
    pytest_sessionstart [hook]
        session: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x1011faa80>
          plugin_name: 4313819776
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x100df67b0>
          plugin_name: pytestconfig
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/mark/__init__.py'>
          plugin_name: mark
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/main.py'>
          plugin_name: main
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/runner.py'>
          plugin_name: runner
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/fixtures.py'>
          plugin_name: fixtures
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/helpconfig.py'>
          plugin_name: helpconfig
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/python.py'>
          plugin_name: python
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/terminal.py'>
          plugin_name: terminal
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/debugging.py'>
          plugin_name: debugging
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/unittest.py'>
          plugin_name: unittest
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/capture.py'>
          plugin_name: capture
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/skipping.py'>
          plugin_name: skipping
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/legacypath.py'>
          plugin_name: legacypath
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/tmpdir.py'>
          plugin_name: tmpdir
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/monkeypatch.py'>
          plugin_name: monkeypatch
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/recwarn.py'>
          plugin_name: recwarn
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/pastebin.py'>
          plugin_name: pastebin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/__init__.py'>
          plugin_name: assertion
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/junitxml.py'>
          plugin_name: junitxml
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/doctest.py'>
          plugin_name: doctest
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/cacheprovider.py'>
          plugin_name: cacheprovider
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/freeze_support.py'>
          plugin_name: freeze_support
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/setuponly.py'>
          plugin_name: setuponly
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/setupplan.py'>
          plugin_name: setupplan
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/stepwise.py'>
          plugin_name: stepwise
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/warnings.py'>
          plugin_name: warnings
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/logging.py'>
          plugin_name: logging
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/reports.py'>
          plugin_name: reports
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/python_path.py'>
          plugin_name: python_path
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/unraisableexception.py'>
          plugin_name: unraisableexception
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/threadexception.py'>
          plugin_name: threadexception
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/faulthandler.py'>
          plugin_name: faulthandler
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_cov.plugin' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest_cov/plugin.py'>
          plugin_name: pytest_cov
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          plugin_name: capturemanager
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <pytest_cov.plugin.CovPlugin object at 0x101628140>
          plugin_name: _cov
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'conftest' from '/Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py'>
          plugin_name: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          plugin_name: session
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x1073ec440>
          plugin_name: lfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x1052f5dc0>
          plugin_name: nfplugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          plugin_name: legacypath-tmpdir
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x1073edeb0>
          plugin_name: terminalreporter
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x1073eda30>
          plugin_name: logging-plugin
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.fixtures.FixtureManager object at 0x10732acc0>
          plugin_name: funcmanage
          manager: <_pytest.config.PytestPluginManager object at 0x1011faa80>
      finish pytest_plugin_registered --> [] [hook]
      pytest_report_header [hook]
          config: <_pytest.config.Config object at 0x100df67b0>
          start_path: /Users/milanfusco/PycharmProjects/McFramework
          startdir: /Users/milanfusco/PycharmProjects/McFramework
      early skip of rewriting module: email.parser [assertion]
      early skip of rewriting module: email.feedparser [assertion]
      finish pytest_report_header --> [['rootdir: /Users/milanfusco/PycharmProjects/McFramework', 'configfile: pyproject.toml', 'plugins: cov-7.0.0'], ['using: pytest-8.2.0', 'setuptools registered plugins:', '  pytest-cov-7.0.0 at /Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest_cov/plugin.py']] [hook]
    finish pytest_sessionstart --> [] [hook]
    pytest_collection [hook]
        session: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
    perform_collect <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0> ['tests/'] [collection]
        pytest_collectstart [hook]
            collector: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        finish pytest_collectstart --> [] [hook]
        pytest_make_collect_report [hook]
            collector: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        processing argument CollectionArgument(path=PosixPath('/Users/milanfusco/PycharmProjects/McFramework/tests'), parts=[], module_name=None) [collection]
            pytest_collect_directory [hook]
                path: /Users/milanfusco/PycharmProjects/McFramework
                parent: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
            finish pytest_collect_directory --> <Dir McFramework> [hook]
            pytest_collectstart [hook]
                collector: <Dir McFramework>
            finish pytest_collectstart --> [] [hook]
            pytest_make_collect_report [hook]
                collector: <Dir McFramework>
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.DS_Store
                  path: /Users/milanfusco/PycharmProjects/McFramework/.DS_Store
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/.DS_Store
                  path: /Users/milanfusco/PycharmProjects/McFramework/.DS_Store
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.git
                  path: /Users/milanfusco/PycharmProjects/McFramework/.git
              finish pytest_ignore_collect --> True [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.gitignore
                  path: /Users/milanfusco/PycharmProjects/McFramework/.gitignore
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/.gitignore
                  path: /Users/milanfusco/PycharmProjects/McFramework/.gitignore
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.idea
                  path: /Users/milanfusco/PycharmProjects/McFramework/.idea
              finish pytest_ignore_collect --> True [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.pytest_cache
                  path: /Users/milanfusco/PycharmProjects/McFramework/.pytest_cache
              finish pytest_ignore_collect --> True [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.ruff_cache
                  path: /Users/milanfusco/PycharmProjects/McFramework/.ruff_cache
              finish pytest_ignore_collect --> True [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/.venv
                  path: /Users/milanfusco/PycharmProjects/McFramework/.venv
              finish pytest_ignore_collect --> True [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/LICENSE 
                  path: /Users/milanfusco/PycharmProjects/McFramework/LICENSE 
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/LICENSE 
                  path: /Users/milanfusco/PycharmProjects/McFramework/LICENSE 
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/README.md
                  path: /Users/milanfusco/PycharmProjects/McFramework/README.md
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/README.md
                  path: /Users/milanfusco/PycharmProjects/McFramework/README.md
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/coverage
                  path: /Users/milanfusco/PycharmProjects/McFramework/coverage
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_directory [hook]
                  path: /Users/milanfusco/PycharmProjects/McFramework/coverage
                  parent: <Dir McFramework>
              finish pytest_collect_directory --> <Dir coverage> [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/coverage.xml
                  path: /Users/milanfusco/PycharmProjects/McFramework/coverage.xml
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/coverage.xml
                  path: /Users/milanfusco/PycharmProjects/McFramework/coverage.xml
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/demo.py
                  path: /Users/milanfusco/PycharmProjects/McFramework/demo.py
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/demo.py
                  path: /Users/milanfusco/PycharmProjects/McFramework/demo.py
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/docs
                  path: /Users/milanfusco/PycharmProjects/McFramework/docs
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_directory [hook]
                  path: /Users/milanfusco/PycharmProjects/McFramework/docs
                  parent: <Dir McFramework>
              finish pytest_collect_directory --> <Dir docs> [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/img
                  path: /Users/milanfusco/PycharmProjects/McFramework/img
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_directory [hook]
                  path: /Users/milanfusco/PycharmProjects/McFramework/img
                  parent: <Dir McFramework>
              finish pytest_collect_directory --> <Dir img> [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/pyproject.toml
                  path: /Users/milanfusco/PycharmProjects/McFramework/pyproject.toml
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/pyproject.toml
                  path: /Users/milanfusco/PycharmProjects/McFramework/pyproject.toml
              finish pytest_collect_file --> [] [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/src
                  path: /Users/milanfusco/PycharmProjects/McFramework/src
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_directory [hook]
                  path: /Users/milanfusco/PycharmProjects/McFramework/src
                  parent: <Dir McFramework>
              finish pytest_collect_directory --> <Dir src> [hook]
              pytest_collect_directory [hook]
                  path: /Users/milanfusco/PycharmProjects/McFramework/tests
                  parent: <Dir McFramework>
              finish pytest_collect_directory --> <Dir tests> [hook]
              pytest_ignore_collect [hook]
                  config: <_pytest.config.Config object at 0x100df67b0>
                  collection_path: /Users/milanfusco/PycharmProjects/McFramework/trace
                  path: /Users/milanfusco/PycharmProjects/McFramework/trace
              finish pytest_ignore_collect --> None [hook]
              pytest_collect_file [hook]
                  parent: <Dir McFramework>
                  file_path: /Users/milanfusco/PycharmProjects/McFramework/trace
                  path: /Users/milanfusco/PycharmProjects/McFramework/trace
              finish pytest_collect_file --> [] [hook]
            finish pytest_make_collect_report --> <CollectReport '.' lenresult=5 outcome='passed'> [hook]
        finish pytest_make_collect_report --> <CollectReport '' lenresult=1 outcome='passed'> [hook]
        pytest_collectreport [hook]
            report: <CollectReport '' lenresult=1 outcome='passed'>
        finish pytest_collectreport --> [] [hook]
    genitems <Dir tests> [collection]
      pytest_collectstart [hook]
          collector: <Dir tests>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Dir tests>
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/.coverage
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/.coverage
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/.coverage
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/.coverage
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/__pycache__
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/__pycache__
        finish pytest_ignore_collect --> True [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/conftest.py
        finish pytest_collect_file --> [] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py
          finish pytest_pycollect_makemodule --> <Module test_basic_statistics.py> [hook]
        finish pytest_collect_file --> [<Module test_basic_statistics.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py
          finish pytest_pycollect_makemodule --> <Module test_confidence_intervals.py> [hook]
        finish pytest_collect_file --> [<Module test_confidence_intervals.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py
          finish pytest_pycollect_makemodule --> <Module test_core.py> [hook]
        finish pytest_collect_file --> [<Module test_core.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py
          finish pytest_pycollect_makemodule --> <Module test_distribution_free_metrics.py> [hook]
        finish pytest_collect_file --> [<Module test_distribution_free_metrics.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py
          finish pytest_pycollect_makemodule --> <Module test_errors_and_edge_cases.py> [hook]
        finish pytest_collect_file --> [<Module test_errors_and_edge_cases.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py
          finish pytest_pycollect_makemodule --> <Module test_integration.py> [hook]
        finish pytest_collect_file --> [<Module test_integration.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py
          finish pytest_pycollect_makemodule --> <Module test_mocking_and_isolation.py> [hook]
        finish pytest_collect_file --> [<Module test_mocking_and_isolation.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py
          finish pytest_pycollect_makemodule --> <Module test_parametrized.py> [hook]
        finish pytest_collect_file --> [<Module test_parametrized.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py
          finish pytest_pycollect_makemodule --> <Module test_performance_and_concurrency.py> [hook]
        finish pytest_collect_file --> [<Module test_performance_and_concurrency.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py
          finish pytest_pycollect_makemodule --> <Module test_regression.py> [hook]
        finish pytest_collect_file --> [<Module test_regression.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py
          finish pytest_pycollect_makemodule --> <Module test_sims.py> [hook]
        finish pytest_collect_file --> [<Module test_sims.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py
          finish pytest_pycollect_makemodule --> <Module test_stats_engine.py> [hook]
        finish pytest_collect_file --> [<Module test_stats_engine.py>] [hook]
        pytest_ignore_collect [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            collection_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
        finish pytest_ignore_collect --> None [hook]
        pytest_collect_file [hook]
            parent: <Dir tests>
            file_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
            path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
          pytest_pycollect_makemodule [hook]
              parent: <Dir tests>
              module_path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
              path: /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py
          finish pytest_pycollect_makemodule --> <Module test_utils.py> [hook]
        finish pytest_collect_file --> [<Module test_utils.py>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests' lenresult=13 outcome='passed'> [hook]
    genitems <Module test_basic_statistics.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_basic_statistics.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_basic_statistics.py>
      find_module called for: test_basic_statistics [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_basic_statistics.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: kurtosis
            obj: <function kurtosis at 0x10732dda0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: mean
            obj: <function mean at 0x10732d760>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: percentiles
            obj: <function percentiles at 0x10732dc60>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: skew
            obj: <function skew at 0x10732dd00>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: std
            obj: <function std at 0x10732dbc0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_statistics.py>
            name: TestBasicStatistics
            obj: <class 'test_basic_statistics.TestBasicStatistics'>
        finish pytest_pycollect_makeitem --> <Class TestBasicStatistics> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_basic_statistics.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestBasicStatistics> [collection]
      pytest_collectstart [hook]
          collector: <Class TestBasicStatistics>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestBasicStatistics>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_mean_simple
            obj: <function TestBasicStatistics.test_mean_simple at 0x107446480>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_mean_simple>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_mean_with_nan_policy_omit
            obj: <function TestBasicStatistics.test_mean_with_nan_policy_omit at 0x1074463e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1073ede80>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_mean_with_nan_policy_omit>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_std_simple
            obj: <function TestBasicStatistics.test_std_simple at 0x107446520>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_std_simple>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_std_single_value
            obj: <function TestBasicStatistics.test_std_single_value at 0x1074465c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_std_single_value>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_std_empty_array
            obj: <function TestBasicStatistics.test_std_empty_array at 0x107446660>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_std_empty_array>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_percentiles_default
            obj: <function TestBasicStatistics.test_percentiles_default at 0x107446700>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_percentiles_default>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_percentiles_with_nan_policy
            obj: <function TestBasicStatistics.test_percentiles_with_nan_policy at 0x1074467a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_percentiles_with_nan_policy>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_skew_normal_distribution
            obj: <function TestBasicStatistics.test_skew_normal_distribution at 0x107446840>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_skew_normal_distribution>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_skew_small_sample
            obj: <function TestBasicStatistics.test_skew_small_sample at 0x1074468e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_skew_small_sample>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_kurtosis_normal_distribution
            obj: <function TestBasicStatistics.test_kurtosis_normal_distribution at 0x107446980>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_kurtosis_normal_distribution>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestBasicStatistics>
            name: test_kurtosis_small_sample
            obj: <function TestBasicStatistics.test_kurtosis_small_sample at 0x107446a20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1070460f0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_kurtosis_small_sample>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_basic_statistics.py::TestBasicStatistics' lenresult=11 outcome='passed'> [hook]
    genitems <Function test_mean_simple> [collection]
      pytest_itemcollected [hook]
          item: <Function test_mean_simple>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_mean_with_nan_policy_omit> [collection]
      pytest_itemcollected [hook]
          item: <Function test_mean_with_nan_policy_omit>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_std_simple> [collection]
      pytest_itemcollected [hook]
          item: <Function test_std_simple>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_std_single_value> [collection]
      pytest_itemcollected [hook]
          item: <Function test_std_single_value>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_std_empty_array> [collection]
      pytest_itemcollected [hook]
          item: <Function test_std_empty_array>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_percentiles_default> [collection]
      pytest_itemcollected [hook]
          item: <Function test_percentiles_default>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_percentiles_with_nan_policy> [collection]
      pytest_itemcollected [hook]
          item: <Function test_percentiles_with_nan_policy>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_skew_normal_distribution> [collection]
      pytest_itemcollected [hook]
          item: <Function test_skew_normal_distribution>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_skew_small_sample> [collection]
      pytest_itemcollected [hook]
          item: <Function test_skew_small_sample>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_kurtosis_normal_distribution> [collection]
      pytest_itemcollected [hook]
          item: <Function test_kurtosis_normal_distribution>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_kurtosis_small_sample> [collection]
      pytest_itemcollected [hook]
          item: <Function test_kurtosis_small_sample>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_basic_statistics.py::TestBasicStatistics' lenresult=11 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_basic_statistics.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_confidence_intervals.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_confidence_intervals.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_confidence_intervals.py>
      find_module called for: test_confidence_intervals [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_confidence_intervals.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: ci_mean
            obj: <function ci_mean at 0x10732de40>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: ci_mean_chebyshev
            obj: <function ci_mean_chebyshev at 0x10732df80>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_confidence_intervals.py>
            name: TestConfidenceIntervals
            obj: <class 'test_confidence_intervals.TestConfidenceIntervals'>
        finish pytest_pycollect_makeitem --> <Class TestConfidenceIntervals> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_confidence_intervals.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestConfidenceIntervals> [collection]
      pytest_collectstart [hook]
          collector: <Class TestConfidenceIntervals>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestConfidenceIntervals>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestConfidenceIntervals>
            name: test_ci_mean_basic
            obj: <function TestConfidenceIntervals.test_ci_mean_basic at 0x107447ce0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10744a7e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_ci_mean_basic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestConfidenceIntervals>
            name: test_ci_mean_contains_true_mean
            obj: <function TestConfidenceIntervals.test_ci_mean_contains_true_mean at 0x107447d80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10744a7e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_ci_mean_contains_true_mean>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestConfidenceIntervals>
            name: test_ci_mean_small_sample
            obj: <function TestConfidenceIntervals.test_ci_mean_small_sample at 0x107447e20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10744a7e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_ci_mean_small_sample>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestConfidenceIntervals>
            name: test_ci_mean_chebyshev
            obj: <function TestConfidenceIntervals.test_ci_mean_chebyshev at 0x107447ec0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10744a7e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_ci_mean_chebyshev>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestConfidenceIntervals>
            name: test_ci_mean_chebyshev_wider_than_normal
            obj: <function TestConfidenceIntervals.test_ci_mean_chebyshev_wider_than_normal at 0x107447f60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10744a7e0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_ci_mean_chebyshev_wider_than_normal>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals' lenresult=5 outcome='passed'> [hook]
    genitems <Function test_ci_mean_basic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_ci_mean_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_ci_mean_contains_true_mean> [collection]
      pytest_itemcollected [hook]
          item: <Function test_ci_mean_contains_true_mean>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_ci_mean_small_sample> [collection]
      pytest_itemcollected [hook]
          item: <Function test_ci_mean_small_sample>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_ci_mean_chebyshev> [collection]
      pytest_itemcollected [hook]
          item: <Function test_ci_mean_chebyshev>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_ci_mean_chebyshev_wider_than_normal> [collection]
      pytest_itemcollected [hook]
          item: <Function test_ci_mean_chebyshev_wider_than_normal>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals' lenresult=5 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_confidence_intervals.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_core.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_core.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_core.py>
      find_module called for: test_core [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_core.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: MonteCarloFramework
            obj: <class 'mcframework.core.MonteCarloFramework'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: MonteCarloSimulation
            obj: <class 'mcframework.core.MonteCarloSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: SimulationResult
            obj: <class 'mcframework.core.SimulationResult'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: make_blocks
            obj: <function make_blocks at 0x10732e340>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: PiEstimationSimulation
            obj: <class 'mcframework.sims.PiEstimationSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestMakeBlocks
            obj: <class 'test_core.TestMakeBlocks'>
        finish pytest_pycollect_makeitem --> <Class TestMakeBlocks> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestSimulationResult
            obj: <class 'test_core.TestSimulationResult'>
        finish pytest_pycollect_makeitem --> <Class TestSimulationResult> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestMonteCarloSimulation
            obj: <class 'test_core.TestMonteCarloSimulation'>
        finish pytest_pycollect_makeitem --> <Class TestMonteCarloSimulation> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestMonteCarloFramework
            obj: <class 'test_core.TestMonteCarloFramework'>
        finish pytest_pycollect_makeitem --> <Class TestMonteCarloFramework> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestMetadataFields
            obj: <class 'test_core.TestMetadataFields'>
        finish pytest_pycollect_makeitem --> <Class TestMetadataFields> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestParallelBackend
            obj: <class 'test_core.TestParallelBackend'>
        finish pytest_pycollect_makeitem --> <Class TestParallelBackend> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestParallelFallback
            obj: <class 'test_core.TestParallelFallback'>
        finish pytest_pycollect_makeitem --> <Class TestParallelFallback> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestThreadBackendExecution
            obj: <class 'test_core.TestThreadBackendExecution'>
        finish pytest_pycollect_makeitem --> <Class TestThreadBackendExecution> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestSeedSequenceGeneration
            obj: <class 'test_core.TestSeedSequenceGeneration'>
        finish pytest_pycollect_makeitem --> <Class TestSeedSequenceGeneration> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestKeyboardInterruptHandling
            obj: <class 'test_core.TestKeyboardInterruptHandling'>
        finish pytest_pycollect_makeitem --> <Class TestKeyboardInterruptHandling> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_core.py>
            name: TestAdditionalEdgeCases
            obj: <class 'test_core.TestAdditionalEdgeCases'>
        finish pytest_pycollect_makeitem --> <Class TestAdditionalEdgeCases> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py' lenresult=11 outcome='passed'> [hook]
    genitems <Class TestMakeBlocks> [collection]
      pytest_collectstart [hook]
          collector: <Class TestMakeBlocks>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestMakeBlocks>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMakeBlocks>
            name: test_make_blocks_exact_division
            obj: <function TestMakeBlocks.test_make_blocks_exact_division at 0x1074a4680>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_make_blocks_exact_division>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMakeBlocks>
            name: test_make_blocks_with_remainder
            obj: <function TestMakeBlocks.test_make_blocks_with_remainder at 0x1074a4720>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_make_blocks_with_remainder>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMakeBlocks>
            name: test_make_blocks_small_n
            obj: <function TestMakeBlocks.test_make_blocks_small_n at 0x1074a47c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_make_blocks_small_n>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMakeBlocks>
            name: test_make_blocks_coverage
            obj: <function TestMakeBlocks.test_make_blocks_coverage at 0x1074a4860>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_make_blocks_coverage>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestMakeBlocks' lenresult=4 outcome='passed'> [hook]
    genitems <Function test_make_blocks_exact_division> [collection]
      pytest_itemcollected [hook]
          item: <Function test_make_blocks_exact_division>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_make_blocks_with_remainder> [collection]
      pytest_itemcollected [hook]
          item: <Function test_make_blocks_with_remainder>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_make_blocks_small_n> [collection]
      pytest_itemcollected [hook]
          item: <Function test_make_blocks_small_n>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_make_blocks_coverage> [collection]
      pytest_itemcollected [hook]
          item: <Function test_make_blocks_coverage>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestMakeBlocks' lenresult=4 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestSimulationResult> [collection]
      pytest_collectstart [hook]
          collector: <Class TestSimulationResult>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestSimulationResult>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestSimulationResult>
            name: test_simulation_result_creation
            obj: <function TestSimulationResult.test_simulation_result_creation at 0x1074a4900>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a87d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simulation_result_creation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestSimulationResult>
            name: test_result_to_string_basic
            obj: <function TestSimulationResult.test_result_to_string_basic at 0x1074a49a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a87d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_result_to_string_basic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestSimulationResult>
            name: test_result_to_string_with_metadata
            obj: <function TestSimulationResult.test_result_to_string_with_metadata at 0x1074a4a40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107449c40>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_result_to_string_with_metadata>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestSimulationResult' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_simulation_result_creation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simulation_result_creation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_result_to_string_basic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_result_to_string_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_result_to_string_with_metadata> [collection]
      pytest_itemcollected [hook]
          item: <Function test_result_to_string_with_metadata>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestSimulationResult' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestMonteCarloSimulation> [collection]
      pytest_collectstart [hook]
          collector: <Class TestMonteCarloSimulation>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestMonteCarloSimulation>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_simulation_initialization
            obj: <function TestMonteCarloSimulation.test_simulation_initialization at 0x1074a4ae0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simulation_initialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_set_seed
            obj: <function TestMonteCarloSimulation.test_set_seed at 0x1074a4b80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_set_seed>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_sequential_basic
            obj: <function TestMonteCarloSimulation.test_run_sequential_basic at 0x1074a4c20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_sequential_basic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_with_progress_callback
            obj: <function TestMonteCarloSimulation.test_run_with_progress_callback at 0x1074a4cc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_with_progress_callback>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_with_custom_percentiles
            obj: <function TestMonteCarloSimulation.test_run_with_custom_percentiles at 0x1074a4d60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_with_custom_percentiles>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_with_stats_engine
            obj: <function TestMonteCarloSimulation.test_run_with_stats_engine at 0x1074a4e00>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_with_stats_engine>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_parallel_basic
            obj: <function TestMonteCarloSimulation.test_run_parallel_basic at 0x1074a4ea0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_parallel_basic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_sequential_vs_parallel_reproducibility
            obj: <function TestMonteCarloSimulation.test_run_sequential_vs_parallel_reproducibility at 0x1074a4f40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_sequential_vs_parallel_reproducibility>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_run_invalid_n_simulations
            obj: <function TestMonteCarloSimulation.test_run_invalid_n_simulations at 0x1074a4fe0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_invalid_n_simulations>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_serialization
            obj: <function TestMonteCarloSimulation.test_serialization at 0x1074a5080>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_serialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloSimulation>
            name: test_deterministic_results
            obj: <function TestMonteCarloSimulation.test_deterministic_results at 0x1074a5120>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8740>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_deterministic_results>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestMonteCarloSimulation' lenresult=11 outcome='passed'> [hook]
    genitems <Function test_simulation_initialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simulation_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_set_seed> [collection]
      pytest_itemcollected [hook]
          item: <Function test_set_seed>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_sequential_basic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_sequential_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_with_progress_callback> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_with_progress_callback>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_with_custom_percentiles> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_with_custom_percentiles>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_with_stats_engine> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_with_stats_engine>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_parallel_basic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_parallel_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_sequential_vs_parallel_reproducibility> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_sequential_vs_parallel_reproducibility>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_invalid_n_simulations> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_invalid_n_simulations>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_serialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_serialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_deterministic_results> [collection]
      pytest_itemcollected [hook]
          item: <Function test_deterministic_results>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestMonteCarloSimulation' lenresult=11 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestMonteCarloFramework> [collection]
      pytest_collectstart [hook]
          collector: <Class TestMonteCarloFramework>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestMonteCarloFramework>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_framework_initialization
            obj: <function TestMonteCarloFramework.test_framework_initialization at 0x1074a51c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_framework_initialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_register_simulation
            obj: <function TestMonteCarloFramework.test_register_simulation at 0x1074a5260>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_register_simulation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_register_simulation_custom_name
            obj: <function TestMonteCarloFramework.test_register_simulation_custom_name at 0x1074a5300>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_register_simulation_custom_name>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_run_simulation
            obj: <function TestMonteCarloFramework.test_run_simulation at 0x1074a53a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_simulation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_run_simulation_not_found
            obj: <function TestMonteCarloFramework.test_run_simulation_not_found at 0x1074a5440>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_simulation_not_found>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_mean
            obj: <function TestMonteCarloFramework.test_compare_results_mean at 0x1074a54e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_mean>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_std
            obj: <function TestMonteCarloFramework.test_compare_results_std at 0x1074a5580>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_std>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_percentile
            obj: <function TestMonteCarloFramework.test_compare_results_percentile at 0x1074a5620>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_percentile>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_no_results
            obj: <function TestMonteCarloFramework.test_compare_results_no_results at 0x1074a56c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_no_results>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_invalid_metric
            obj: <function TestMonteCarloFramework.test_compare_results_invalid_metric at 0x1074a5760>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_invalid_metric>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: test_compare_results_percentile_not_in_percentiles_dict
            obj: <function TestMonteCarloFramework.test_compare_results_percentile_not_in_percentiles_dict at 0x1074a5800>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8800>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_percentile_not_in_percentiles_dict>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMonteCarloFramework>
            name: TestPercentileMerging
            obj: <class 'test_core.TestMonteCarloFramework.TestPercentileMerging'>
        finish pytest_pycollect_makeitem --> <Class TestPercentileMerging> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestMonteCarloFramework' lenresult=12 outcome='passed'> [hook]
    genitems <Function test_framework_initialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_framework_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_register_simulation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_register_simulation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_register_simulation_custom_name> [collection]
      pytest_itemcollected [hook]
          item: <Function test_register_simulation_custom_name>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_simulation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_simulation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_simulation_not_found> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_simulation_not_found>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_mean> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_mean>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_std> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_std>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_percentile> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_percentile>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_no_results> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_no_results>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_invalid_metric> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_invalid_metric>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_percentile_not_in_percentiles_dict> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_percentile_not_in_percentiles_dict>
      finish pytest_itemcollected --> [] [hook]
    genitems <Class TestPercentileMerging> [collection]
      pytest_collectstart [hook]
          collector: <Class TestPercentileMerging>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestPercentileMerging>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPercentileMerging>
            name: test_stats_engine_percentiles_merge
            obj: <function TestMonteCarloFramework.TestPercentileMerging.test_stats_engine_percentiles_merge at 0x1074a5940>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074aa5a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_stats_engine_percentiles_merge>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging' lenresult=1 outcome='passed'> [hook]
    genitems <Function test_stats_engine_percentiles_merge> [collection]
      pytest_itemcollected [hook]
          item: <Function test_stats_engine_percentiles_merge>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestMonteCarloFramework' lenresult=12 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestMetadataFields> [collection]
      pytest_collectstart [hook]
          collector: <Class TestMetadataFields>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestMetadataFields>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMetadataFields>
            name: test_metadata_includes_requested_percentiles
            obj: <function TestMetadataFields.test_metadata_includes_requested_percentiles at 0x1074a58a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8770>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_metadata_includes_requested_percentiles>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMetadataFields>
            name: test_metadata_includes_engine_defaults_used
            obj: <function TestMetadataFields.test_metadata_includes_engine_defaults_used at 0x1074a59e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8770>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_metadata_includes_engine_defaults_used>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMetadataFields>
            name: test_metadata_without_requested_percentiles
            obj: <function TestMetadataFields.test_metadata_without_requested_percentiles at 0x1074a5a80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074a8770>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_metadata_without_requested_percentiles>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestMetadataFields' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_metadata_includes_requested_percentiles> [collection]
      pytest_itemcollected [hook]
          item: <Function test_metadata_includes_requested_percentiles>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_metadata_includes_engine_defaults_used> [collection]
      pytest_itemcollected [hook]
          item: <Function test_metadata_includes_engine_defaults_used>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_metadata_without_requested_percentiles> [collection]
      pytest_itemcollected [hook]
          item: <Function test_metadata_without_requested_percentiles>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestMetadataFields' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestParallelBackend> [collection]
      pytest_collectstart [hook]
          collector: <Class TestParallelBackend>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestParallelBackend>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParallelBackend>
            name: test_parallel_backend_thread_explicit
            obj: <function TestParallelBackend.test_parallel_backend_thread_explicit at 0x1074a5b20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074aa5a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_backend_thread_explicit>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParallelBackend>
            name: test_parallel_backend_process_explicit
            obj: <function TestParallelBackend.test_parallel_backend_process_explicit at 0x1074a5bc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074aa5a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_backend_process_explicit>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParallelBackend>
            name: test_parallel_backend_auto_uses_threads
            obj: <function TestParallelBackend.test_parallel_backend_auto_uses_threads at 0x1074a5c60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074aa5a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_backend_auto_uses_threads>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestParallelBackend' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_parallel_backend_thread_explicit> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_backend_thread_explicit>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parallel_backend_process_explicit> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_backend_process_explicit>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parallel_backend_auto_uses_threads> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_backend_auto_uses_threads>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestParallelBackend' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestParallelFallback> [collection]
      pytest_collectstart [hook]
          collector: <Class TestParallelFallback>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestParallelFallback>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParallelFallback>
            name: test_parallel_fallback_small_n_simulations
            obj: <function TestParallelFallback.test_parallel_fallback_small_n_simulations at 0x1074a5d00>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c40b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_fallback_small_n_simulations>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParallelFallback>
            name: test_parallel_fallback_single_worker
            obj: <function TestParallelFallback.test_parallel_fallback_single_worker at 0x1074a5da0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c40b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_fallback_single_worker>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestParallelFallback' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_parallel_fallback_small_n_simulations> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_fallback_small_n_simulations>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parallel_fallback_single_worker> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_fallback_single_worker>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestParallelFallback' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestThreadBackendExecution> [collection]
      pytest_collectstart [hook]
          collector: <Class TestThreadBackendExecution>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestThreadBackendExecution>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestThreadBackendExecution>
            name: test_default_backend_is_auto
            obj: <function TestThreadBackendExecution.test_default_backend_is_auto at 0x1074a5e40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c40b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_default_backend_is_auto>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestThreadBackendExecution>
            name: test_thread_backend_with_large_job
            obj: <function TestThreadBackendExecution.test_thread_backend_with_large_job at 0x1074a5ee0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c40b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_thread_backend_with_large_job>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestThreadBackendExecution' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_default_backend_is_auto> [collection]
      pytest_itemcollected [hook]
          item: <Function test_default_backend_is_auto>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_thread_backend_with_large_job> [collection]
      pytest_itemcollected [hook]
          item: <Function test_thread_backend_with_large_job>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestThreadBackendExecution' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestSeedSequenceGeneration> [collection]
      pytest_collectstart [hook]
          collector: <Class TestSeedSequenceGeneration>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestSeedSequenceGeneration>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestSeedSequenceGeneration>
            name: test_parallel_without_seed_generates_random_sequences
            obj: <function TestSeedSequenceGeneration.test_parallel_without_seed_generates_random_sequences at 0x1074a5f80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c40b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_without_seed_generates_random_sequences>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestSeedSequenceGeneration' lenresult=1 outcome='passed'> [hook]
    genitems <Function test_parallel_without_seed_generates_random_sequences> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_without_seed_generates_random_sequences>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestSeedSequenceGeneration' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestKeyboardInterruptHandling> [collection]
      pytest_collectstart [hook]
          collector: <Class TestKeyboardInterruptHandling>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestKeyboardInterruptHandling>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestKeyboardInterruptHandling>
            name: test_keyboard_interrupt_cleanup
            obj: <function TestKeyboardInterruptHandling.test_keyboard_interrupt_cleanup at 0x1074a6020>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c44d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_keyboard_interrupt_cleanup>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestKeyboardInterruptHandling' lenresult=1 outcome='passed'> [hook]
    genitems <Function test_keyboard_interrupt_cleanup> [collection]
      pytest_itemcollected [hook]
          item: <Function test_keyboard_interrupt_cleanup>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestKeyboardInterruptHandling' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestAdditionalEdgeCases> [collection]
      pytest_collectstart [hook]
          collector: <Class TestAdditionalEdgeCases>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestAdditionalEdgeCases>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestAdditionalEdgeCases>
            name: test_run_without_percentiles_and_without_stats
            obj: <function TestAdditionalEdgeCases.test_run_without_percentiles_and_without_stats at 0x1074a60c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c44d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_without_percentiles_and_without_stats>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestAdditionalEdgeCases>
            name: test_run_with_empty_percentiles_list
            obj: <function TestAdditionalEdgeCases.test_run_with_empty_percentiles_list at 0x1074a6160>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c44d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_with_empty_percentiles_list>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestAdditionalEdgeCases>
            name: test_parallel_with_custom_block_size
            obj: <function TestAdditionalEdgeCases.test_parallel_with_custom_block_size at 0x1074a6200>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074abf50>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_with_custom_block_size>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestAdditionalEdgeCases>
            name: test_compare_results_with_all_metrics
            obj: <function TestAdditionalEdgeCases.test_compare_results_with_all_metrics at 0x1074a62a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074abf50>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_compare_results_with_all_metrics>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_core.py::TestAdditionalEdgeCases' lenresult=4 outcome='passed'> [hook]
    genitems <Function test_run_without_percentiles_and_without_stats> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_without_percentiles_and_without_stats>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_with_empty_percentiles_list> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_with_empty_percentiles_list>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parallel_with_custom_block_size> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_with_custom_block_size>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_compare_results_with_all_metrics> [collection]
      pytest_itemcollected [hook]
          item: <Function test_compare_results_with_all_metrics>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py::TestAdditionalEdgeCases' lenresult=4 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_core.py' lenresult=11 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_distribution_free_metrics.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_distribution_free_metrics.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_distribution_free_metrics.py>
      find_module called for: test_distribution_free_metrics [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_distribution_free_metrics.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: bias_to_target
            obj: <function bias_to_target at 0x10732e160>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: chebyshev_required_n
            obj: <function chebyshev_required_n at 0x10732e020>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: markov_error_prob
            obj: <function markov_error_prob at 0x10732e0c0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: mse_to_target
            obj: <function mse_to_target at 0x10732e200>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_distribution_free_metrics.py>
            name: TestDistributionFreeMetrics
            obj: <class 'test_distribution_free_metrics.TestDistributionFreeMetrics'>
        finish pytest_pycollect_makeitem --> <Class TestDistributionFreeMetrics> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_distribution_free_metrics.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestDistributionFreeMetrics> [collection]
      pytest_collectstart [hook]
          collector: <Class TestDistributionFreeMetrics>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestDistributionFreeMetrics>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_chebyshev_required_n
            obj: <function TestDistributionFreeMetrics.test_chebyshev_required_n at 0x1074a45e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_chebyshev_required_n>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_chebyshev_required_n_no_eps
            obj: <function TestDistributionFreeMetrics.test_chebyshev_required_n_no_eps at 0x1074a6980>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_chebyshev_required_n_no_eps>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_markov_error_prob
            obj: <function TestDistributionFreeMetrics.test_markov_error_prob at 0x1074a6340>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_markov_error_prob>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_markov_error_prob_no_target
            obj: <function TestDistributionFreeMetrics.test_markov_error_prob_no_target at 0x1074a67a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_markov_error_prob_no_target>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_bias_to_target
            obj: <function TestDistributionFreeMetrics.test_bias_to_target at 0x1074a63e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_bias_to_target>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestDistributionFreeMetrics>
            name: test_mse_to_target
            obj: <function TestDistributionFreeMetrics.test_mse_to_target at 0x1074a65c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c5130>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_mse_to_target>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics' lenresult=6 outcome='passed'> [hook]
    genitems <Function test_chebyshev_required_n> [collection]
      pytest_itemcollected [hook]
          item: <Function test_chebyshev_required_n>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_chebyshev_required_n_no_eps> [collection]
      pytest_itemcollected [hook]
          item: <Function test_chebyshev_required_n_no_eps>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_markov_error_prob> [collection]
      pytest_itemcollected [hook]
          item: <Function test_markov_error_prob>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_markov_error_prob_no_target> [collection]
      pytest_itemcollected [hook]
          item: <Function test_markov_error_prob_no_target>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_bias_to_target> [collection]
      pytest_itemcollected [hook]
          item: <Function test_bias_to_target>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_mse_to_target> [collection]
      pytest_itemcollected [hook]
          item: <Function test_mse_to_target>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics' lenresult=6 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_distribution_free_metrics.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_errors_and_edge_cases.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_errors_and_edge_cases.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_errors_and_edge_cases.py>
      find_module called for: test_errors_and_edge_cases [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_errors_and_edge_cases.py [assertion]
      early skip of rewriting module: unittest.mock [assertion]
      early skip of rewriting module: asyncio [assertion]
      early skip of rewriting module: asyncio.base_events [assertion]
      early skip of rewriting module: ssl [assertion]
      early skip of rewriting module: _ssl [assertion]
      early skip of rewriting module: asyncio.constants [assertion]
      early skip of rewriting module: asyncio.coroutines [assertion]
      early skip of rewriting module: asyncio.events [assertion]
      early skip of rewriting module: asyncio.format_helpers [assertion]
      early skip of rewriting module: _asyncio [assertion]
      early skip of rewriting module: asyncio.base_futures [assertion]
      early skip of rewriting module: asyncio.exceptions [assertion]
      early skip of rewriting module: asyncio.base_tasks [assertion]
      early skip of rewriting module: asyncio.futures [assertion]
      early skip of rewriting module: asyncio.protocols [assertion]
      early skip of rewriting module: asyncio.sslproto [assertion]
      early skip of rewriting module: asyncio.transports [assertion]
      early skip of rewriting module: asyncio.log [assertion]
      early skip of rewriting module: asyncio.staggered [assertion]
      early skip of rewriting module: asyncio.locks [assertion]
      early skip of rewriting module: asyncio.mixins [assertion]
      early skip of rewriting module: asyncio.tasks [assertion]
      early skip of rewriting module: asyncio.timeouts [assertion]
      early skip of rewriting module: asyncio.trsock [assertion]
      early skip of rewriting module: asyncio.runners [assertion]
      early skip of rewriting module: asyncio.queues [assertion]
      early skip of rewriting module: asyncio.streams [assertion]
      early skip of rewriting module: asyncio.subprocess [assertion]
      early skip of rewriting module: asyncio.taskgroups [assertion]
      early skip of rewriting module: asyncio.threads [assertion]
      early skip of rewriting module: asyncio.unix_events [assertion]
      early skip of rewriting module: asyncio.base_subprocess [assertion]
      early skip of rewriting module: asyncio.selector_events [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: Mock
            obj: <class 'unittest.mock.Mock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: PortfolioSimulation
            obj: <class 'mcframework.sims.PortfolioSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: StatsEngine
            obj: <class 'mcframework.stats_engine.StatsEngine'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: ci_mean
            obj: <function ci_mean at 0x10732de40>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: mean
            obj: <function mean at 0x10732d760>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: TestEdgeCases
            obj: <class 'test_errors_and_edge_cases.TestEdgeCases'>
        finish pytest_pycollect_makeitem --> <Class TestEdgeCases> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_errors_and_edge_cases.py>
            name: TestErrorHandling
            obj: <class 'test_errors_and_edge_cases.TestErrorHandling'>
        finish pytest_pycollect_makeitem --> <Class TestErrorHandling> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_errors_and_edge_cases.py' lenresult=2 outcome='passed'> [hook]
    genitems <Class TestEdgeCases> [collection]
      pytest_collectstart [hook]
          collector: <Class TestEdgeCases>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestEdgeCases>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_single_simulation_run
            obj: <function TestEdgeCases.test_single_simulation_run at 0x1074a7a60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_single_simulation_run>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_very_small_confidence_interval
            obj: <function TestEdgeCases.test_very_small_confidence_interval at 0x1074a6c00>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_very_small_confidence_interval>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_very_high_confidence_interval
            obj: <function TestEdgeCases.test_very_high_confidence_interval at 0x107681760>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_very_high_confidence_interval>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_negative_portfolio_values_possible
            obj: <function TestEdgeCases.test_negative_portfolio_values_possible at 0x107681b20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_negative_portfolio_values_possible>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_zero_year_portfolio
            obj: <function TestEdgeCases.test_zero_year_portfolio at 0x107681bc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_zero_year_portfolio>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_nan_handling_in_stats
            obj: <function TestEdgeCases.test_nan_handling_in_stats at 0x107681c60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_nan_handling_in_stats>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_empty_percentiles_list
            obj: <function TestEdgeCases.test_empty_percentiles_list at 0x107681d00>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_empty_percentiles_list>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestEdgeCases>
            name: test_duplicate_percentiles
            obj: <function TestEdgeCases.test_duplicate_percentiles at 0x107681da0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1074c7b30>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_duplicate_percentiles>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases' lenresult=8 outcome='passed'> [hook]
    genitems <Function test_single_simulation_run> [collection]
      pytest_itemcollected [hook]
          item: <Function test_single_simulation_run>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_very_small_confidence_interval> [collection]
      pytest_itemcollected [hook]
          item: <Function test_very_small_confidence_interval>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_very_high_confidence_interval> [collection]
      pytest_itemcollected [hook]
          item: <Function test_very_high_confidence_interval>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_negative_portfolio_values_possible> [collection]
      pytest_itemcollected [hook]
          item: <Function test_negative_portfolio_values_possible>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_zero_year_portfolio> [collection]
      pytest_itemcollected [hook]
          item: <Function test_zero_year_portfolio>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_nan_handling_in_stats> [collection]
      pytest_itemcollected [hook]
          item: <Function test_nan_handling_in_stats>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_empty_percentiles_list> [collection]
      pytest_itemcollected [hook]
          item: <Function test_empty_percentiles_list>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_duplicate_percentiles> [collection]
      pytest_itemcollected [hook]
          item: <Function test_duplicate_percentiles>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases' lenresult=8 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestErrorHandling> [collection]
      pytest_collectstart [hook]
          collector: <Class TestErrorHandling>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestErrorHandling>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_invalid_simulation_name_in_framework
            obj: <function TestErrorHandling.test_invalid_simulation_name_in_framework at 0x107681e40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_invalid_simulation_name_in_framework>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_negative_n_simulations
            obj: <function TestErrorHandling.test_negative_n_simulations at 0x107681ee0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_negative_n_simulations>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_zero_n_simulations
            obj: <function TestErrorHandling.test_zero_n_simulations at 0x107681f80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_zero_n_simulations>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_invalid_percentile_metric_comparison
            obj: <function TestErrorHandling.test_invalid_percentile_metric_comparison at 0x107682020>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_invalid_percentile_metric_comparison>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_stats_engine_with_empty_metrics
            obj: <function TestErrorHandling.test_stats_engine_with_empty_metrics at 0x1076820c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_stats_engine_with_empty_metrics>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestErrorHandling>
            name: test_simulation_with_failed_stats
            obj: <function TestErrorHandling.test_simulation_with_failed_stats at 0x107682160>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x107669fd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simulation_with_failed_stats>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling' lenresult=6 outcome='passed'> [hook]
    genitems <Function test_invalid_simulation_name_in_framework> [collection]
      pytest_itemcollected [hook]
          item: <Function test_invalid_simulation_name_in_framework>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_negative_n_simulations> [collection]
      pytest_itemcollected [hook]
          item: <Function test_negative_n_simulations>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_zero_n_simulations> [collection]
      pytest_itemcollected [hook]
          item: <Function test_zero_n_simulations>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_invalid_percentile_metric_comparison> [collection]
      pytest_itemcollected [hook]
          item: <Function test_invalid_percentile_metric_comparison>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_stats_engine_with_empty_metrics> [collection]
      pytest_itemcollected [hook]
          item: <Function test_stats_engine_with_empty_metrics>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_simulation_with_failed_stats> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simulation_with_failed_stats>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling' lenresult=6 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_errors_and_edge_cases.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_integration.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_integration.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_integration.py>
      find_module called for: test_integration [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_integration.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: MonteCarloFramework
            obj: <class 'mcframework.core.MonteCarloFramework'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: MonteCarloSimulation
            obj: <class 'mcframework.core.MonteCarloSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: PiEstimationSimulation
            obj: <class 'mcframework.sims.PiEstimationSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: PortfolioSimulation
            obj: <class 'mcframework.sims.PortfolioSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_integration.py>
            name: TestIntegration
            obj: <class 'test_integration.TestIntegration'>
        finish pytest_pycollect_makeitem --> <Class TestIntegration> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_integration.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestIntegration> [collection]
      pytest_collectstart [hook]
          collector: <Class TestIntegration>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestIntegration>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_end_to_end_pi_estimation
            obj: <function TestIntegration.test_end_to_end_pi_estimation at 0x107682200>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_end_to_end_pi_estimation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_end_to_end_portfolio
            obj: <function TestIntegration.test_end_to_end_portfolio at 0x107682340>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_end_to_end_portfolio>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_multiple_simulations_comparison
            obj: <function TestIntegration.test_multiple_simulations_comparison at 0x107682480>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_multiple_simulations_comparison>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_parallel_execution_consistency
            obj: <function TestIntegration.test_parallel_execution_consistency at 0x107682520>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_execution_consistency>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_large_scale_simulation
            obj: <function TestIntegration.test_large_scale_simulation at 0x1076825c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_large_scale_simulation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_custom_simulation_integration
            obj: <function TestIntegration.test_custom_simulation_integration at 0x107682660>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_custom_simulation_integration>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestIntegration>
            name: test_simulation_kwargs_passthrough
            obj: <function TestIntegration.test_simulation_kwargs_passthrough at 0x107682700>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766aab0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simulation_kwargs_passthrough>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_integration.py::TestIntegration' lenresult=7 outcome='passed'> [hook]
    genitems <Function test_end_to_end_pi_estimation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_end_to_end_pi_estimation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_end_to_end_portfolio> [collection]
      pytest_itemcollected [hook]
          item: <Function test_end_to_end_portfolio>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_multiple_simulations_comparison> [collection]
      pytest_itemcollected [hook]
          item: <Function test_multiple_simulations_comparison>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_parallel_execution_consistency> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_execution_consistency>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_large_scale_simulation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_large_scale_simulation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_custom_simulation_integration> [collection]
      pytest_itemcollected [hook]
          item: <Function test_custom_simulation_integration>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_simulation_kwargs_passthrough> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simulation_kwargs_passthrough>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_integration.py::TestIntegration' lenresult=7 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_integration.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_mocking_and_isolation.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_mocking_and_isolation.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_mocking_and_isolation.py>
      find_module called for: test_mocking_and_isolation [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_mocking_and_isolation.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: Mock
            obj: <class 'unittest.mock.Mock'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: MonteCarloSimulation
            obj: <class 'mcframework.core.MonteCarloSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: SimulationResult
            obj: <class 'mcframework.core.SimulationResult'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_mocking_and_isolation.py>
            name: TestMocking
            obj: <class 'test_mocking_and_isolation.TestMocking'>
        finish pytest_pycollect_makeitem --> <Class TestMocking> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_mocking_and_isolation.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestMocking> [collection]
      pytest_collectstart [hook]
          collector: <Class TestMocking>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestMocking>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMocking>
            name: test_simulation_with_mocked_rng
            obj: <function TestMocking.test_simulation_with_mocked_rng at 0x107682a20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766bbc0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_simulation_with_mocked_rng>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMocking>
            name: test_framework_with_mocked_simulation
            obj: <function TestMocking.test_framework_with_mocked_simulation at 0x107682ac0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766bbc0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_framework_with_mocked_simulation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestMocking>
            name: test_progress_callback_mock
            obj: <function TestMocking.test_progress_callback_mock at 0x107682b60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766bbc0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_progress_callback_mock>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_mocking_and_isolation.py::TestMocking' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_simulation_with_mocked_rng> [collection]
      pytest_itemcollected [hook]
          item: <Function test_simulation_with_mocked_rng>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_framework_with_mocked_simulation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_framework_with_mocked_simulation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_progress_callback_mock> [collection]
      pytest_itemcollected [hook]
          item: <Function test_progress_callback_mock>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_mocking_and_isolation.py::TestMocking' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_mocking_and_isolation.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_parametrized.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_parametrized.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_parametrized.py>
      find_module called for: test_parametrized [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_parametrized.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: make_blocks
            obj: <function make_blocks at 0x10732e340>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: autocrit
            obj: <function autocrit at 0x10732d440>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: ci_mean
            obj: <function ci_mean at 0x10732de40>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_parametrized.py>
            name: TestParametrized
            obj: <class 'test_parametrized.TestParametrized'>
        finish pytest_pycollect_makeitem --> <Class TestParametrized> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_parametrized.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestParametrized> [collection]
      pytest_collectstart [hook]
          collector: <Class TestParametrized>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestParametrized>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParametrized>
            name: test_various_simulation_sizes
            obj: <function TestParametrized.test_various_simulation_sizes at 0x1074472e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769c8c0>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 1
                argname: n_sims
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 10
                argname: n_sims
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 100
                argname: n_sims
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 1000
                argname: n_sims
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_various_simulation_sizes[1]>, <Function test_various_simulation_sizes[10]>, <Function test_various_simulation_sizes[100]>, <Function test_various_simulation_sizes[1000]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParametrized>
            name: test_various_confidence_levels
            obj: <function TestParametrized.test_various_confidence_levels at 0x1074471a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766a120>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 0.9
                argname: confidence
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 0.95
                argname: confidence
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 0.99
                argname: confidence
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_various_confidence_levels[0.9]>, <Function test_various_confidence_levels[0.95]>, <Function test_various_confidence_levels[0.99]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParametrized>
            name: test_various_ci_methods
            obj: <function TestParametrized.test_various_ci_methods at 0x107446f20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766a120>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: z
                argname: method
            finish pytest_make_parametrize_id --> None [hook]
          early skip of rewriting module: encodings.unicode_escape [assertion]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: t
                argname: method
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: auto
                argname: method
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_various_ci_methods[z]>, <Function test_various_ci_methods[t]>, <Function test_various_ci_methods[auto]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParametrized>
            name: test_various_worker_counts
            obj: <function TestParametrized.test_various_worker_counts at 0x1076827a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766a120>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 1
                argname: n_workers
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 2
                argname: n_workers
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 4
                argname: n_workers
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_various_worker_counts[1]>, <Function test_various_worker_counts[2]>, <Function test_various_worker_counts[4]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestParametrized>
            name: test_various_block_sizes
            obj: <function TestParametrized.test_various_block_sizes at 0x1076828e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10766a120>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 100
                argname: block_size
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 1000
                argname: block_size
            finish pytest_make_parametrize_id --> None [hook]
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x100df67b0>
                val: 10000
                argname: block_size
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_various_block_sizes[100]>, <Function test_various_block_sizes[1000]>, <Function test_various_block_sizes[10000]>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_parametrized.py::TestParametrized' lenresult=16 outcome='passed'> [hook]
    genitems <Function test_various_simulation_sizes[1]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_simulation_sizes[1]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_simulation_sizes[10]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_simulation_sizes[10]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_simulation_sizes[100]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_simulation_sizes[100]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_simulation_sizes[1000]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_simulation_sizes[1000]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_confidence_levels[0.9]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_confidence_levels[0.9]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_confidence_levels[0.95]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_confidence_levels[0.95]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_confidence_levels[0.99]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_confidence_levels[0.99]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_ci_methods[z]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_ci_methods[z]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_ci_methods[t]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_ci_methods[t]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_ci_methods[auto]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_ci_methods[auto]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_worker_counts[1]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_worker_counts[1]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_worker_counts[2]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_worker_counts[2]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_worker_counts[4]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_worker_counts[4]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_block_sizes[100]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_block_sizes[100]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_block_sizes[1000]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_block_sizes[1000]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_various_block_sizes[10000]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_various_block_sizes[10000]>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_parametrized.py::TestParametrized' lenresult=16 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_parametrized.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_performance_and_concurrency.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_performance_and_concurrency.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_performance_and_concurrency.py>
      find_module called for: test_performance_and_concurrency [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_performance_and_concurrency.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: time
            obj: <module 'time' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: _worker_run_chunk
            obj: <function _worker_run_chunk at 0x1031409a0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_performance_and_concurrency.py>
            name: TestPerformance
            obj: <class 'test_performance_and_concurrency.TestPerformance'>
        finish pytest_pycollect_makeitem --> <Class TestPerformance> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_performance_and_concurrency.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestPerformance> [collection]
      pytest_collectstart [hook]
          collector: <Class TestPerformance>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestPerformance>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPerformance>
            name: test_parallel_faster_than_sequential
            obj: <function TestPerformance.test_parallel_faster_than_sequential at 0x107682e80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769dfa0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_parallel_faster_than_sequential>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPerformance>
            name: test_memory_efficiency_streaming
            obj: <function TestPerformance.test_memory_efficiency_streaming at 0x107683060>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769dfa0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_memory_efficiency_streaming>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPerformance>
            name: test_worker_run_chunk
            obj: <function TestPerformance.test_worker_run_chunk at 0x107682f20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769dfa0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_worker_run_chunk>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_performance_and_concurrency.py::TestPerformance' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_parallel_faster_than_sequential> [collection]
      pytest_itemcollected [hook]
          item: <Function test_parallel_faster_than_sequential>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_memory_efficiency_streaming> [collection]
      pytest_itemcollected [hook]
          item: <Function test_memory_efficiency_streaming>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_worker_run_chunk> [collection]
      pytest_itemcollected [hook]
          item: <Function test_worker_run_chunk>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_performance_and_concurrency.py::TestPerformance' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_performance_and_concurrency.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_regression.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_regression.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_regression.py>
      find_module called for: test_regression [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_regression.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: PiEstimationSimulation
            obj: <class 'mcframework.sims.PiEstimationSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: PortfolioSimulation
            obj: <class 'mcframework.sims.PortfolioSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_regression.py>
            name: TestRegression
            obj: <class 'test_regression.TestRegression'>
        finish pytest_pycollect_makeitem --> <Class TestRegression> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_regression.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestRegression> [collection]
      pytest_collectstart [hook]
          collector: <Class TestRegression>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestRegression>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestRegression>
            name: test_seed_reproducibility_across_runs
            obj: <function TestRegression.test_seed_reproducibility_across_runs at 0x1076836a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e8d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_seed_reproducibility_across_runs>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestRegression>
            name: test_stats_engine_percentile_merge
            obj: <function TestRegression.test_stats_engine_percentile_merge at 0x107683740>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e8d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_stats_engine_percentile_merge>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestRegression>
            name: test_portfolio_gbm_vs_non_gbm_consistency
            obj: <function TestRegression.test_portfolio_gbm_vs_non_gbm_consistency at 0x1076837e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e8d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_gbm_vs_non_gbm_consistency>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_regression.py::TestRegression' lenresult=3 outcome='passed'> [hook]
    genitems <Function test_seed_reproducibility_across_runs> [collection]
      pytest_itemcollected [hook]
          item: <Function test_seed_reproducibility_across_runs>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_stats_engine_percentile_merge> [collection]
      pytest_itemcollected [hook]
          item: <Function test_stats_engine_percentile_merge>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_portfolio_gbm_vs_non_gbm_consistency> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_regression.py::TestRegression' lenresult=3 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_regression.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_sims.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_sims.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_sims.py>
      find_module called for: test_sims [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_sims.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: PiEstimationSimulation
            obj: <class 'mcframework.sims.PiEstimationSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: PortfolioSimulation
            obj: <class 'mcframework.sims.PortfolioSimulation'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: TestPiEstimationSimulation
            obj: <class 'test_sims.TestPiEstimationSimulation'>
        finish pytest_pycollect_makeitem --> <Class TestPiEstimationSimulation> [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_sims.py>
            name: TestPortfolioSimulation
            obj: <class 'test_sims.TestPortfolioSimulation'>
        finish pytest_pycollect_makeitem --> <Class TestPortfolioSimulation> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_sims.py' lenresult=2 outcome='passed'> [hook]
    genitems <Class TestPiEstimationSimulation> [collection]
      pytest_collectstart [hook]
          collector: <Class TestPiEstimationSimulation>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestPiEstimationSimulation>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPiEstimationSimulation>
            name: test_pi_estimation_initialization
            obj: <function TestPiEstimationSimulation.test_pi_estimation_initialization at 0x107683ce0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769ede0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_pi_estimation_initialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPiEstimationSimulation>
            name: test_pi_estimation_single_run
            obj: <function TestPiEstimationSimulation.test_pi_estimation_single_run at 0x107683d80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769ede0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_pi_estimation_single_run>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPiEstimationSimulation>
            name: test_pi_estimation_convergence
            obj: <function TestPiEstimationSimulation.test_pi_estimation_convergence at 0x107683e20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769ede0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_pi_estimation_convergence>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPiEstimationSimulation>
            name: test_pi_estimation_antithetic
            obj: <function TestPiEstimationSimulation.test_pi_estimation_antithetic at 0x107683ec0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769ede0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_pi_estimation_antithetic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPiEstimationSimulation>
            name: test_pi_estimation_full_run
            obj: <function TestPiEstimationSimulation.test_pi_estimation_full_run at 0x107683f60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769ede0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_pi_estimation_full_run>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_sims.py::TestPiEstimationSimulation' lenresult=5 outcome='passed'> [hook]
    genitems <Function test_pi_estimation_initialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_pi_estimation_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_pi_estimation_single_run> [collection]
      pytest_itemcollected [hook]
          item: <Function test_pi_estimation_single_run>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_pi_estimation_convergence> [collection]
      pytest_itemcollected [hook]
          item: <Function test_pi_estimation_convergence>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_pi_estimation_antithetic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_pi_estimation_antithetic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_pi_estimation_full_run> [collection]
      pytest_itemcollected [hook]
          item: <Function test_pi_estimation_full_run>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_sims.py::TestPiEstimationSimulation' lenresult=5 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Class TestPortfolioSimulation> [collection]
      pytest_collectstart [hook]
          collector: <Class TestPortfolioSimulation>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestPortfolioSimulation>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPortfolioSimulation>
            name: test_portfolio_initialization
            obj: <function TestPortfolioSimulation.test_portfolio_initialization at 0x1076bc040>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769f770>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_initialization>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPortfolioSimulation>
            name: test_portfolio_single_run_basic
            obj: <function TestPortfolioSimulation.test_portfolio_single_run_basic at 0x1076bc0e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e7b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_single_run_basic>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPortfolioSimulation>
            name: test_portfolio_positive_return_on_average
            obj: <function TestPortfolioSimulation.test_portfolio_positive_return_on_average at 0x1076bc180>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e7b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_positive_return_on_average>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPortfolioSimulation>
            name: test_portfolio_non_gbm
            obj: <function TestPortfolioSimulation.test_portfolio_non_gbm at 0x1076bc220>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e7b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_non_gbm>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestPortfolioSimulation>
            name: test_portfolio_zero_volatility
            obj: <function TestPortfolioSimulation.test_portfolio_zero_volatility at 0x1076bc2c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769e7b0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_portfolio_zero_volatility>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_sims.py::TestPortfolioSimulation' lenresult=5 outcome='passed'> [hook]
    genitems <Function test_portfolio_initialization> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_initialization>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_portfolio_single_run_basic> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_single_run_basic>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_portfolio_positive_return_on_average> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_positive_return_on_average>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_portfolio_non_gbm> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_non_gbm>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_portfolio_zero_volatility> [collection]
      pytest_itemcollected [hook]
          item: <Function test_portfolio_zero_volatility>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_sims.py::TestPortfolioSimulation' lenresult=5 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_sims.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_stats_engine.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_stats_engine.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_stats_engine.py>
      find_module called for: test_stats_engine [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_stats_engine.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: np
            obj: <module 'numpy' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/numpy/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: FnMetric
            obj: <class 'mcframework.stats_engine.FnMetric'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: StatsEngine
            obj: <class 'mcframework.stats_engine.StatsEngine'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: build_default_engine
            obj: <function build_default_engine at 0x10732e2a0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: mean
            obj: <function mean at 0x10732d760>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: std
            obj: <function std at 0x10732dbc0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_stats_engine.py>
            name: TestStatsEngine
            obj: <class 'test_stats_engine.TestStatsEngine'>
        finish pytest_pycollect_makeitem --> <Class TestStatsEngine> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_stats_engine.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestStatsEngine> [collection]
      pytest_collectstart [hook]
          collector: <Class TestStatsEngine>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestStatsEngine>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_engine_creation
            obj: <function TestStatsEngine.test_engine_creation at 0x1076bca40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c44a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_engine_creation>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_engine_compute
            obj: <function TestStatsEngine.test_engine_compute at 0x1076bcae0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c44a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_engine_compute>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_default_engine_build
            obj: <function TestStatsEngine.test_default_engine_build at 0x1076bcb80>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c44a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_default_engine_build>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_default_engine_compute
            obj: <function TestStatsEngine.test_default_engine_compute at 0x1076bcc20>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c44a0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_default_engine_compute>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_engine_without_dist_free
            obj: <function TestStatsEngine.test_engine_without_dist_free at 0x1076bccc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769fc50>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_engine_without_dist_free>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestStatsEngine>
            name: test_engine_without_target_bounds
            obj: <function TestStatsEngine.test_engine_without_target_bounds at 0x1076bcd60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10769f680>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_engine_without_target_bounds>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_stats_engine.py::TestStatsEngine' lenresult=6 outcome='passed'> [hook]
    genitems <Function test_engine_creation> [collection]
      pytest_itemcollected [hook]
          item: <Function test_engine_creation>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_engine_compute> [collection]
      pytest_itemcollected [hook]
          item: <Function test_engine_compute>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_default_engine_build> [collection]
      pytest_itemcollected [hook]
          item: <Function test_default_engine_build>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_default_engine_compute> [collection]
      pytest_itemcollected [hook]
          item: <Function test_default_engine_compute>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_engine_without_dist_free> [collection]
      pytest_itemcollected [hook]
          item: <Function test_engine_without_dist_free>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_engine_without_target_bounds> [collection]
      pytest_itemcollected [hook]
          item: <Function test_engine_without_target_bounds>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_stats_engine.py::TestStatsEngine' lenresult=6 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_stats_engine.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_utils.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_utils.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_utils.py>
      find_module called for: test_utils [assertion]
      matched test file '/Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py' [assertion]
      found cached rewritten pyc for /Users/milanfusco/PycharmProjects/McFramework/tests/test_utils.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: pytest
            obj: <module 'pytest' from '/Users/milanfusco/PycharmProjects/McFramework/.venv/lib/python3.12/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: autocrit
            obj: <function autocrit at 0x10732d440>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: t_crit
            obj: <function t_crit at 0x10732d3a0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: z_crit
            obj: <function z_crit at 0x10732ce00>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_utils.py>
            name: TestCriticalValues
            obj: <class 'test_utils.TestCriticalValues'>
        finish pytest_pycollect_makeitem --> <Class TestCriticalValues> [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_utils.py' lenresult=1 outcome='passed'> [hook]
    genitems <Class TestCriticalValues> [collection]
      pytest_collectstart [hook]
          collector: <Class TestCriticalValues>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Class TestCriticalValues>
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_z_crit_95_confidence
            obj: <function TestCriticalValues.test_z_crit_95_confidence at 0x1076bc900>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_z_crit_95_confidence>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_z_crit_99_confidence
            obj: <function TestCriticalValues.test_z_crit_99_confidence at 0x1076bce00>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_z_crit_99_confidence>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_z_crit_invalid_confidence_too_low
            obj: <function TestCriticalValues.test_z_crit_invalid_confidence_too_low at 0x1076bcea0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_z_crit_invalid_confidence_too_low>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_z_crit_invalid_confidence_too_high
            obj: <function TestCriticalValues.test_z_crit_invalid_confidence_too_high at 0x1076bcf40>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_z_crit_invalid_confidence_too_high>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_t_crit_95_confidence_30_df
            obj: <function TestCriticalValues.test_t_crit_95_confidence_30_df at 0x1076bcfe0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_t_crit_95_confidence_30_df>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_t_crit_small_df
            obj: <function TestCriticalValues.test_t_crit_small_df at 0x1076bd080>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_t_crit_small_df>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_t_crit_invalid_df
            obj: <function TestCriticalValues.test_t_crit_invalid_df at 0x1076bd120>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_t_crit_invalid_df>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_autocrit_large_n_uses_z
            obj: <function TestCriticalValues.test_autocrit_large_n_uses_z at 0x1076bd1c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_autocrit_large_n_uses_z>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_autocrit_small_n_uses_t
            obj: <function TestCriticalValues.test_autocrit_small_n_uses_t at 0x1076bd260>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_autocrit_small_n_uses_t>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_autocrit_force_z
            obj: <function TestCriticalValues.test_autocrit_force_z at 0x1076bd300>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_autocrit_force_z>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_autocrit_force_t
            obj: <function TestCriticalValues.test_autocrit_force_t at 0x1076bd3a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_autocrit_force_t>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Class TestCriticalValues>
            name: test_autocrit_invalid_method
            obj: <function TestCriticalValues.test_autocrit_invalid_method at 0x1076bd440>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x1076c4dd0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_autocrit_invalid_method>] [hook]
      finish pytest_make_collect_report --> <CollectReport 'tests/test_utils.py::TestCriticalValues' lenresult=12 outcome='passed'> [hook]
    genitems <Function test_z_crit_95_confidence> [collection]
      pytest_itemcollected [hook]
          item: <Function test_z_crit_95_confidence>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_z_crit_99_confidence> [collection]
      pytest_itemcollected [hook]
          item: <Function test_z_crit_99_confidence>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_z_crit_invalid_confidence_too_low> [collection]
      pytest_itemcollected [hook]
          item: <Function test_z_crit_invalid_confidence_too_low>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_z_crit_invalid_confidence_too_high> [collection]
      pytest_itemcollected [hook]
          item: <Function test_z_crit_invalid_confidence_too_high>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_t_crit_95_confidence_30_df> [collection]
      pytest_itemcollected [hook]
          item: <Function test_t_crit_95_confidence_30_df>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_t_crit_small_df> [collection]
      pytest_itemcollected [hook]
          item: <Function test_t_crit_small_df>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_t_crit_invalid_df> [collection]
      pytest_itemcollected [hook]
          item: <Function test_t_crit_invalid_df>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_autocrit_large_n_uses_z> [collection]
      pytest_itemcollected [hook]
          item: <Function test_autocrit_large_n_uses_z>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_autocrit_small_n_uses_t> [collection]
      pytest_itemcollected [hook]
          item: <Function test_autocrit_small_n_uses_t>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_autocrit_force_z> [collection]
      pytest_itemcollected [hook]
          item: <Function test_autocrit_force_z>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_autocrit_force_t> [collection]
      pytest_itemcollected [hook]
          item: <Function test_autocrit_force_t>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_autocrit_invalid_method> [collection]
      pytest_itemcollected [hook]
          item: <Function test_autocrit_invalid_method>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_utils.py::TestCriticalValues' lenresult=12 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests/test_utils.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'tests' lenresult=13 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collection_modifyitems [hook]
          session: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          config: <_pytest.config.Config object at 0x100df67b0>
          items: [<Function test_mean_simple>, <Function test_mean_with_nan_policy_omit>, <Function test_std_simple>, <Function test_std_single_value>, <Function test_std_empty_array>, <Function test_percentiles_default>, <Function test_percentiles_with_nan_policy>, <Function test_skew_normal_distribution>, <Function test_skew_small_sample>, <Function test_kurtosis_normal_distribution>, <Function test_kurtosis_small_sample>, <Function test_ci_mean_basic>, <Function test_ci_mean_contains_true_mean>, <Function test_ci_mean_small_sample>, <Function test_ci_mean_chebyshev>, <Function test_ci_mean_chebyshev_wider_than_normal>, <Function test_make_blocks_exact_division>, <Function test_make_blocks_with_remainder>, <Function test_make_blocks_small_n>, <Function test_make_blocks_coverage>, <Function test_simulation_result_creation>, <Function test_result_to_string_basic>, <Function test_result_to_string_with_metadata>, <Function test_simulation_initialization>, <Function test_set_seed>, <Function test_run_sequential_basic>, <Function test_run_with_progress_callback>, <Function test_run_with_custom_percentiles>, <Function test_run_with_stats_engine>, <Function test_run_parallel_basic>, <Function test_run_sequential_vs_parallel_reproducibility>, <Function test_run_invalid_n_simulations>, <Function test_serialization>, <Function test_deterministic_results>, <Function test_framework_initialization>, <Function test_register_simulation>, <Function test_register_simulation_custom_name>, <Function test_run_simulation>, <Function test_run_simulation_not_found>, <Function test_compare_results_mean>, <Function test_compare_results_std>, <Function test_compare_results_percentile>, <Function test_compare_results_no_results>, <Function test_compare_results_invalid_metric>, <Function test_compare_results_percentile_not_in_percentiles_dict>, <Function test_stats_engine_percentiles_merge>, <Function test_metadata_includes_requested_percentiles>, <Function test_metadata_includes_engine_defaults_used>, <Function test_metadata_without_requested_percentiles>, <Function test_parallel_backend_thread_explicit>, <Function test_parallel_backend_process_explicit>, <Function test_parallel_backend_auto_uses_threads>, <Function test_parallel_fallback_small_n_simulations>, <Function test_parallel_fallback_single_worker>, <Function test_default_backend_is_auto>, <Function test_thread_backend_with_large_job>, <Function test_parallel_without_seed_generates_random_sequences>, <Function test_keyboard_interrupt_cleanup>, <Function test_run_without_percentiles_and_without_stats>, <Function test_run_with_empty_percentiles_list>, <Function test_parallel_with_custom_block_size>, <Function test_compare_results_with_all_metrics>, <Function test_chebyshev_required_n>, <Function test_chebyshev_required_n_no_eps>, <Function test_markov_error_prob>, <Function test_markov_error_prob_no_target>, <Function test_bias_to_target>, <Function test_mse_to_target>, <Function test_single_simulation_run>, <Function test_very_small_confidence_interval>, <Function test_very_high_confidence_interval>, <Function test_negative_portfolio_values_possible>, <Function test_zero_year_portfolio>, <Function test_nan_handling_in_stats>, <Function test_empty_percentiles_list>, <Function test_duplicate_percentiles>, <Function test_invalid_simulation_name_in_framework>, <Function test_negative_n_simulations>, <Function test_zero_n_simulations>, <Function test_invalid_percentile_metric_comparison>, <Function test_stats_engine_with_empty_metrics>, <Function test_simulation_with_failed_stats>, <Function test_end_to_end_pi_estimation>, <Function test_end_to_end_portfolio>, <Function test_multiple_simulations_comparison>, <Function test_parallel_execution_consistency>, <Function test_large_scale_simulation>, <Function test_custom_simulation_integration>, <Function test_simulation_kwargs_passthrough>, <Function test_simulation_with_mocked_rng>, <Function test_framework_with_mocked_simulation>, <Function test_progress_callback_mock>, <Function test_various_simulation_sizes[1]>, <Function test_various_simulation_sizes[10]>, <Function test_various_simulation_sizes[100]>, <Function test_various_simulation_sizes[1000]>, <Function test_various_confidence_levels[0.9]>, <Function test_various_confidence_levels[0.95]>, <Function test_various_confidence_levels[0.99]>, <Function test_various_ci_methods[z]>, <Function test_various_ci_methods[t]>, <Function test_various_ci_methods[auto]>, <Function test_various_worker_counts[1]>, <Function test_various_worker_counts[2]>, <Function test_various_worker_counts[4]>, <Function test_various_block_sizes[100]>, <Function test_various_block_sizes[1000]>, <Function test_various_block_sizes[10000]>, <Function test_parallel_faster_than_sequential>, <Function test_memory_efficiency_streaming>, <Function test_worker_run_chunk>, <Function test_seed_reproducibility_across_runs>, <Function test_stats_engine_percentile_merge>, <Function test_portfolio_gbm_vs_non_gbm_consistency>, <Function test_pi_estimation_initialization>, <Function test_pi_estimation_single_run>, <Function test_pi_estimation_convergence>, <Function test_pi_estimation_antithetic>, <Function test_pi_estimation_full_run>, <Function test_portfolio_initialization>, <Function test_portfolio_single_run_basic>, <Function test_portfolio_positive_return_on_average>, <Function test_portfolio_non_gbm>, <Function test_portfolio_zero_volatility>, <Function test_engine_creation>, <Function test_engine_compute>, <Function test_default_engine_build>, <Function test_default_engine_compute>, <Function test_engine_without_dist_free>, <Function test_engine_without_target_bounds>, <Function test_z_crit_95_confidence>, <Function test_z_crit_99_confidence>, <Function test_z_crit_invalid_confidence_too_low>, <Function test_z_crit_invalid_confidence_too_high>, <Function test_t_crit_95_confidence_30_df>, <Function test_t_crit_small_df>, <Function test_t_crit_invalid_df>, <Function test_autocrit_large_n_uses_z>, <Function test_autocrit_small_n_uses_t>, <Function test_autocrit_force_z>, <Function test_autocrit_force_t>, <Function test_autocrit_invalid_method>]
      finish pytest_collection_modifyitems --> [] [hook]
      pytest_collection_finish [hook]
          session: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        pytest_report_collectionfinish [hook]
            config: <_pytest.config.Config object at 0x100df67b0>
            items: [<Function test_mean_simple>, <Function test_mean_with_nan_policy_omit>, <Function test_std_simple>, <Function test_std_single_value>, <Function test_std_empty_array>, <Function test_percentiles_default>, <Function test_percentiles_with_nan_policy>, <Function test_skew_normal_distribution>, <Function test_skew_small_sample>, <Function test_kurtosis_normal_distribution>, <Function test_kurtosis_small_sample>, <Function test_ci_mean_basic>, <Function test_ci_mean_contains_true_mean>, <Function test_ci_mean_small_sample>, <Function test_ci_mean_chebyshev>, <Function test_ci_mean_chebyshev_wider_than_normal>, <Function test_make_blocks_exact_division>, <Function test_make_blocks_with_remainder>, <Function test_make_blocks_small_n>, <Function test_make_blocks_coverage>, <Function test_simulation_result_creation>, <Function test_result_to_string_basic>, <Function test_result_to_string_with_metadata>, <Function test_simulation_initialization>, <Function test_set_seed>, <Function test_run_sequential_basic>, <Function test_run_with_progress_callback>, <Function test_run_with_custom_percentiles>, <Function test_run_with_stats_engine>, <Function test_run_parallel_basic>, <Function test_run_sequential_vs_parallel_reproducibility>, <Function test_run_invalid_n_simulations>, <Function test_serialization>, <Function test_deterministic_results>, <Function test_framework_initialization>, <Function test_register_simulation>, <Function test_register_simulation_custom_name>, <Function test_run_simulation>, <Function test_run_simulation_not_found>, <Function test_compare_results_mean>, <Function test_compare_results_std>, <Function test_compare_results_percentile>, <Function test_compare_results_no_results>, <Function test_compare_results_invalid_metric>, <Function test_compare_results_percentile_not_in_percentiles_dict>, <Function test_stats_engine_percentiles_merge>, <Function test_metadata_includes_requested_percentiles>, <Function test_metadata_includes_engine_defaults_used>, <Function test_metadata_without_requested_percentiles>, <Function test_parallel_backend_thread_explicit>, <Function test_parallel_backend_process_explicit>, <Function test_parallel_backend_auto_uses_threads>, <Function test_parallel_fallback_small_n_simulations>, <Function test_parallel_fallback_single_worker>, <Function test_default_backend_is_auto>, <Function test_thread_backend_with_large_job>, <Function test_parallel_without_seed_generates_random_sequences>, <Function test_keyboard_interrupt_cleanup>, <Function test_run_without_percentiles_and_without_stats>, <Function test_run_with_empty_percentiles_list>, <Function test_parallel_with_custom_block_size>, <Function test_compare_results_with_all_metrics>, <Function test_chebyshev_required_n>, <Function test_chebyshev_required_n_no_eps>, <Function test_markov_error_prob>, <Function test_markov_error_prob_no_target>, <Function test_bias_to_target>, <Function test_mse_to_target>, <Function test_single_simulation_run>, <Function test_very_small_confidence_interval>, <Function test_very_high_confidence_interval>, <Function test_negative_portfolio_values_possible>, <Function test_zero_year_portfolio>, <Function test_nan_handling_in_stats>, <Function test_empty_percentiles_list>, <Function test_duplicate_percentiles>, <Function test_invalid_simulation_name_in_framework>, <Function test_negative_n_simulations>, <Function test_zero_n_simulations>, <Function test_invalid_percentile_metric_comparison>, <Function test_stats_engine_with_empty_metrics>, <Function test_simulation_with_failed_stats>, <Function test_end_to_end_pi_estimation>, <Function test_end_to_end_portfolio>, <Function test_multiple_simulations_comparison>, <Function test_parallel_execution_consistency>, <Function test_large_scale_simulation>, <Function test_custom_simulation_integration>, <Function test_simulation_kwargs_passthrough>, <Function test_simulation_with_mocked_rng>, <Function test_framework_with_mocked_simulation>, <Function test_progress_callback_mock>, <Function test_various_simulation_sizes[1]>, <Function test_various_simulation_sizes[10]>, <Function test_various_simulation_sizes[100]>, <Function test_various_simulation_sizes[1000]>, <Function test_various_confidence_levels[0.9]>, <Function test_various_confidence_levels[0.95]>, <Function test_various_confidence_levels[0.99]>, <Function test_various_ci_methods[z]>, <Function test_various_ci_methods[t]>, <Function test_various_ci_methods[auto]>, <Function test_various_worker_counts[1]>, <Function test_various_worker_counts[2]>, <Function test_various_worker_counts[4]>, <Function test_various_block_sizes[100]>, <Function test_various_block_sizes[1000]>, <Function test_various_block_sizes[10000]>, <Function test_parallel_faster_than_sequential>, <Function test_memory_efficiency_streaming>, <Function test_worker_run_chunk>, <Function test_seed_reproducibility_across_runs>, <Function test_stats_engine_percentile_merge>, <Function test_portfolio_gbm_vs_non_gbm_consistency>, <Function test_pi_estimation_initialization>, <Function test_pi_estimation_single_run>, <Function test_pi_estimation_convergence>, <Function test_pi_estimation_antithetic>, <Function test_pi_estimation_full_run>, <Function test_portfolio_initialization>, <Function test_portfolio_single_run_basic>, <Function test_portfolio_positive_return_on_average>, <Function test_portfolio_non_gbm>, <Function test_portfolio_zero_volatility>, <Function test_engine_creation>, <Function test_engine_compute>, <Function test_default_engine_build>, <Function test_default_engine_compute>, <Function test_engine_without_dist_free>, <Function test_engine_without_target_bounds>, <Function test_z_crit_95_confidence>, <Function test_z_crit_99_confidence>, <Function test_z_crit_invalid_confidence_too_low>, <Function test_z_crit_invalid_confidence_too_high>, <Function test_t_crit_95_confidence_30_df>, <Function test_t_crit_small_df>, <Function test_t_crit_invalid_df>, <Function test_autocrit_large_n_uses_z>, <Function test_autocrit_small_n_uses_t>, <Function test_autocrit_force_z>, <Function test_autocrit_force_t>, <Function test_autocrit_invalid_method>]
            start_path: /Users/milanfusco/PycharmProjects/McFramework
            startdir: /Users/milanfusco/PycharmProjects/McFramework
        finish pytest_report_collectionfinish --> [] [hook]
      finish pytest_collection_finish --> [] [hook]
    finish pytest_collection --> None [hook]
    pytest_runtestloop [hook]
        session: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=142>
      pytest_runtest_protocol [hook]
          item: <Function test_mean_simple>
          nextitem: <Function test_mean_with_nan_policy_omit>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple
            location: ('tests/test_basic_statistics.py', 9, 'TestBasicStatistics.test_mean_simple')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_mean_simple>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_set_spawn_start_method' scope='session' baseid='tests'>
              request: <SubRequest '_set_spawn_start_method' for <Function test_mean_simple>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mean_simple>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_mean_simple>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_simple>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_mean_simple>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_mean_simple>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_simple>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_mean_simple>
            nextitem: <Function test_mean_with_nan_policy_omit>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_mean_simple>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mean_simple>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_simple>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_mean_simple
            location: ('tests/test_basic_statistics.py', 9, 'TestBasicStatistics.test_mean_simple')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_mean_with_nan_policy_omit>
          nextitem: <Function test_std_simple>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit
            location: ('tests/test_basic_statistics.py', 16, 'TestBasicStatistics.test_mean_with_nan_policy_omit')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_mean_with_nan_policy_omit>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mean_with_nan_policy_omit>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_with_nan_policy_omit>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_mean_with_nan_policy_omit>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_mean_with_nan_policy_omit>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_with_nan_policy_omit>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_mean_with_nan_policy_omit>
            nextitem: <Function test_std_simple>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mean_with_nan_policy_omit>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mean_with_nan_policy_omit>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_mean_with_nan_policy_omit
            location: ('tests/test_basic_statistics.py', 16, 'TestBasicStatistics.test_mean_with_nan_policy_omit')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_std_simple>
          nextitem: <Function test_std_single_value>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple
            location: ('tests/test_basic_statistics.py', 23, 'TestBasicStatistics.test_std_simple')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_std_simple>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_simple>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_std_simple>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_simple>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_std_simple>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_std_simple>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_simple>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_std_simple>
            nextitem: <Function test_std_single_value>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_std_simple>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_simple>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_simple>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_simple
            location: ('tests/test_basic_statistics.py', 23, 'TestBasicStatistics.test_std_simple')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_std_single_value>
          nextitem: <Function test_std_empty_array>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value
            location: ('tests/test_basic_statistics.py', 30, 'TestBasicStatistics.test_std_single_value')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_std_single_value>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_single_value>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_single_value>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_std_single_value>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_std_single_value>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_single_value>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_std_single_value>
            nextitem: <Function test_std_empty_array>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_single_value>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_single_value>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_single_value
            location: ('tests/test_basic_statistics.py', 30, 'TestBasicStatistics.test_std_single_value')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_std_empty_array>
          nextitem: <Function test_percentiles_default>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array
            location: ('tests/test_basic_statistics.py', 37, 'TestBasicStatistics.test_std_empty_array')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_std_empty_array>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_empty_array>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_empty_array>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_std_empty_array>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_std_empty_array>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_empty_array>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_std_empty_array>
            nextitem: <Function test_percentiles_default>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_std_empty_array>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_std_empty_array>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_std_empty_array
            location: ('tests/test_basic_statistics.py', 37, 'TestBasicStatistics.test_std_empty_array')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_percentiles_default>
          nextitem: <Function test_percentiles_with_nan_policy>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default
            location: ('tests/test_basic_statistics.py', 44, 'TestBasicStatistics.test_percentiles_default')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_percentiles_default>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_percentiles_default>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_percentiles_default>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_default>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_percentiles_default>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_percentiles_default>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_default>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_percentiles_default>
            nextitem: <Function test_percentiles_with_nan_policy>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_percentiles_default>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_percentiles_default>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_default>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_default
            location: ('tests/test_basic_statistics.py', 44, 'TestBasicStatistics.test_percentiles_default')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_percentiles_with_nan_policy>
          nextitem: <Function test_skew_normal_distribution>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy
            location: ('tests/test_basic_statistics.py', 53, 'TestBasicStatistics.test_percentiles_with_nan_policy')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_percentiles_with_nan_policy>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_percentiles_with_nan_policy>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_with_nan_policy>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_percentiles_with_nan_policy>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_percentiles_with_nan_policy>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_with_nan_policy>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_percentiles_with_nan_policy>
            nextitem: <Function test_skew_normal_distribution>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_percentiles_with_nan_policy>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_percentiles_with_nan_policy>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_percentiles_with_nan_policy
            location: ('tests/test_basic_statistics.py', 53, 'TestBasicStatistics.test_percentiles_with_nan_policy')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_skew_normal_distribution>
          nextitem: <Function test_skew_small_sample>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution
            location: ('tests/test_basic_statistics.py', 60, 'TestBasicStatistics.test_skew_normal_distribution')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_skew_normal_distribution>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_skew_normal_distribution>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_normal_distribution>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_skew_normal_distribution>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_skew_normal_distribution>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_normal_distribution>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_skew_normal_distribution>
            nextitem: <Function test_skew_small_sample>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_skew_normal_distribution>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_normal_distribution>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_skew_normal_distribution
            location: ('tests/test_basic_statistics.py', 60, 'TestBasicStatistics.test_skew_normal_distribution')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_skew_small_sample>
          nextitem: <Function test_kurtosis_normal_distribution>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample
            location: ('tests/test_basic_statistics.py', 68, 'TestBasicStatistics.test_skew_small_sample')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_skew_small_sample>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_skew_small_sample>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_small_sample>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_skew_small_sample>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_skew_small_sample>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_small_sample>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_skew_small_sample>
            nextitem: <Function test_kurtosis_normal_distribution>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_skew_small_sample>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_skew_small_sample>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_skew_small_sample
            location: ('tests/test_basic_statistics.py', 68, 'TestBasicStatistics.test_skew_small_sample')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_kurtosis_normal_distribution>
          nextitem: <Function test_kurtosis_small_sample>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution
            location: ('tests/test_basic_statistics.py', 75, 'TestBasicStatistics.test_kurtosis_normal_distribution')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_kurtosis_normal_distribution>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_kurtosis_normal_distribution>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_normal_distribution>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_kurtosis_normal_distribution>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_kurtosis_normal_distribution>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_normal_distribution>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_kurtosis_normal_distribution>
            nextitem: <Function test_kurtosis_small_sample>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_kurtosis_normal_distribution>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_normal_distribution>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_normal_distribution
            location: ('tests/test_basic_statistics.py', 75, 'TestBasicStatistics.test_kurtosis_normal_distribution')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_kurtosis_small_sample>
          nextitem: <Function test_ci_mean_basic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample
            location: ('tests/test_basic_statistics.py', 83, 'TestBasicStatistics.test_kurtosis_small_sample')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_kurtosis_small_sample>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_kurtosis_small_sample>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_small_sample>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_kurtosis_small_sample>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_kurtosis_small_sample>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_small_sample>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_kurtosis_small_sample>
            nextitem: <Function test_ci_mean_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_kurtosis_small_sample>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_kurtosis_small_sample>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_basic_statistics.py::TestBasicStatistics::test_kurtosis_small_sample
            location: ('tests/test_basic_statistics.py', 83, 'TestBasicStatistics.test_kurtosis_small_sample')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_ci_mean_basic>
          nextitem: <Function test_ci_mean_contains_true_mean>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic
            location: ('tests/test_confidence_intervals.py', 8, 'TestConfidenceIntervals.test_ci_mean_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_ci_mean_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_basic>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_basic>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_basic>>
          finish pytest_fixture_setup --> [hook]
              n: 1000
              confidence: 0.95
              nan_policy: propagate
              ci_method: auto
              percentiles: (5, 25, 50, 75, 95)
              target: 0.0
              eps: 0.5
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_basic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_ci_mean_basic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_ci_mean_basic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_basic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_ci_mean_basic>
            nextitem: <Function test_ci_mean_contains_true_mean>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_basic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_basic
            location: ('tests/test_confidence_intervals.py', 8, 'TestConfidenceIntervals.test_ci_mean_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_ci_mean_contains_true_mean>
          nextitem: <Function test_ci_mean_small_sample>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean
            location: ('tests/test_confidence_intervals.py', 19, 'TestConfidenceIntervals.test_ci_mean_contains_true_mean')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_ci_mean_contains_true_mean>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_setup --> [hook]
              n: 1000
              confidence: 0.95
              nan_policy: propagate
              ci_method: auto
              percentiles: (5, 25, 50, 75, 95)
              target: 0.0
              eps: 0.5
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_contains_true_mean>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_ci_mean_contains_true_mean>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_ci_mean_contains_true_mean>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_contains_true_mean>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_ci_mean_contains_true_mean>
            nextitem: <Function test_ci_mean_small_sample>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_contains_true_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_contains_true_mean>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_contains_true_mean
            location: ('tests/test_confidence_intervals.py', 19, 'TestConfidenceIntervals.test_ci_mean_contains_true_mean')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_ci_mean_small_sample>
          nextitem: <Function test_ci_mean_chebyshev>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample
            location: ('tests/test_confidence_intervals.py', 25, 'TestConfidenceIntervals.test_ci_mean_small_sample')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_ci_mean_small_sample>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_small_sample>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_small_sample>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_ci_mean_small_sample>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_ci_mean_small_sample>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_small_sample>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_ci_mean_small_sample>
            nextitem: <Function test_ci_mean_chebyshev>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_small_sample>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_small_sample>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_small_sample
            location: ('tests/test_confidence_intervals.py', 25, 'TestConfidenceIntervals.test_ci_mean_small_sample')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_ci_mean_chebyshev>
          nextitem: <Function test_ci_mean_chebyshev_wider_than_normal>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev
            location: ('tests/test_confidence_intervals.py', 32, 'TestConfidenceIntervals.test_ci_mean_chebyshev')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_ci_mean_chebyshev>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_chebyshev>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_chebyshev>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_ci_mean_chebyshev>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_ci_mean_chebyshev>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_ci_mean_chebyshev>
            nextitem: <Function test_ci_mean_chebyshev_wider_than_normal>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_chebyshev>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_chebyshev>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev
            location: ('tests/test_confidence_intervals.py', 32, 'TestConfidenceIntervals.test_ci_mean_chebyshev')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_ci_mean_chebyshev_wider_than_normal>
          nextitem: <Function test_make_blocks_exact_division>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal
            location: ('tests/test_confidence_intervals.py', 40, 'TestConfidenceIntervals.test_ci_mean_chebyshev_wider_than_normal')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_setup --> [hook]
              n: 1000
              confidence: 0.95
              nan_policy: propagate
              ci_method: auto
              percentiles: (5, 25, 50, 75, 95)
              target: 0.0
              eps: 0.5
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_ci_mean_chebyshev_wider_than_normal>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
            nextitem: <Function test_make_blocks_exact_division>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_ci_mean_chebyshev_wider_than_normal>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_ci_mean_chebyshev_wider_than_normal>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_confidence_intervals.py::TestConfidenceIntervals::test_ci_mean_chebyshev_wider_than_normal
            location: ('tests/test_confidence_intervals.py', 40, 'TestConfidenceIntervals.test_ci_mean_chebyshev_wider_than_normal')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_make_blocks_exact_division>
          nextitem: <Function test_make_blocks_with_remainder>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division
            location: ('tests/test_core.py', 11, 'TestMakeBlocks.test_make_blocks_exact_division')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_make_blocks_exact_division>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_exact_division>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_exact_division>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_make_blocks_exact_division>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_make_blocks_exact_division>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_exact_division>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_make_blocks_exact_division>
            nextitem: <Function test_make_blocks_with_remainder>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_exact_division>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_exact_division>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_exact_division
            location: ('tests/test_core.py', 11, 'TestMakeBlocks.test_make_blocks_exact_division')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_make_blocks_with_remainder>
          nextitem: <Function test_make_blocks_small_n>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder
            location: ('tests/test_core.py', 18, 'TestMakeBlocks.test_make_blocks_with_remainder')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_make_blocks_with_remainder>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_with_remainder>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_with_remainder>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_make_blocks_with_remainder>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_make_blocks_with_remainder>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_with_remainder>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_make_blocks_with_remainder>
            nextitem: <Function test_make_blocks_small_n>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_with_remainder>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_with_remainder>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_with_remainder
            location: ('tests/test_core.py', 18, 'TestMakeBlocks.test_make_blocks_with_remainder')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_make_blocks_small_n>
          nextitem: <Function test_make_blocks_coverage>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n
            location: ('tests/test_core.py', 24, 'TestMakeBlocks.test_make_blocks_small_n')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_make_blocks_small_n>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_small_n>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_small_n>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_make_blocks_small_n>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_make_blocks_small_n>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_small_n>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_make_blocks_small_n>
            nextitem: <Function test_make_blocks_coverage>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_small_n>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_small_n>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_small_n
            location: ('tests/test_core.py', 24, 'TestMakeBlocks.test_make_blocks_small_n')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_make_blocks_coverage>
          nextitem: <Function test_simulation_result_creation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage
            location: ('tests/test_core.py', 30, 'TestMakeBlocks.test_make_blocks_coverage')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_make_blocks_coverage>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_coverage>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_coverage>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_make_blocks_coverage>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_make_blocks_coverage>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_coverage>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_make_blocks_coverage>
            nextitem: <Function test_simulation_result_creation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_make_blocks_coverage>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_make_blocks_coverage>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMakeBlocks::test_make_blocks_coverage
            location: ('tests/test_core.py', 30, 'TestMakeBlocks.test_make_blocks_coverage')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simulation_result_creation>
          nextitem: <Function test_result_to_string_basic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_simulation_result_creation
            location: ('tests/test_core.py', 41, 'TestSimulationResult.test_simulation_result_creation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simulation_result_creation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_result_creation>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_result_creation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simulation_result_creation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simulation_result_creation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_result_creation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simulation_result_creation>
            nextitem: <Function test_result_to_string_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_result_creation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_result_creation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_simulation_result_creation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_simulation_result_creation
            location: ('tests/test_core.py', 41, 'TestSimulationResult.test_simulation_result_creation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_result_to_string_basic>
          nextitem: <Function test_result_to_string_with_metadata>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_result_to_string_basic
            location: ('tests/test_core.py', 55, 'TestSimulationResult.test_result_to_string_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_result_to_string_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_result_to_string_basic>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_basic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_result_to_string_basic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_result_to_string_basic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_basic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_result_to_string_basic>
            nextitem: <Function test_result_to_string_with_metadata>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_result_to_string_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_basic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_result_to_string_basic
            location: ('tests/test_core.py', 55, 'TestSimulationResult.test_result_to_string_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_result_to_string_with_metadata>
          nextitem: <Function test_simulation_initialization>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata
            location: ('tests/test_core.py', 71, 'TestSimulationResult.test_result_to_string_with_metadata')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_result_to_string_with_metadata>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_result_to_string_with_metadata>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_with_metadata>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_result_to_string_with_metadata>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_result_to_string_with_metadata>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_with_metadata>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_result_to_string_with_metadata>
            nextitem: <Function test_simulation_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_result_to_string_with_metadata>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_result_to_string_with_metadata>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestSimulationResult::test_result_to_string_with_metadata
            location: ('tests/test_core.py', 71, 'TestSimulationResult.test_result_to_string_with_metadata')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simulation_initialization>
          nextitem: <Function test_set_seed>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization
            location: ('tests/test_core.py', 90, 'TestMonteCarloSimulation.test_simulation_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simulation_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_initialization>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_simulation_initialization>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6900> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_initialization>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simulation_initialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simulation_initialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_initialization>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simulation_initialization>
            nextitem: <Function test_set_seed>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_simulation_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_initialization>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_simulation_initialization
            location: ('tests/test_core.py', 90, 'TestMonteCarloSimulation.test_simulation_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_set_seed>
          nextitem: <Function test_run_sequential_basic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_set_seed
            location: ('tests/test_core.py', 95, 'TestMonteCarloSimulation.test_set_seed')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_set_seed>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_set_seed>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_set_seed>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6810> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_set_seed>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_set_seed>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_set_seed>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_set_seed>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_set_seed>
            nextitem: <Function test_run_sequential_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_set_seed>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_set_seed>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_set_seed>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_set_seed' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_set_seed
            location: ('tests/test_core.py', 95, 'TestMonteCarloSimulation.test_set_seed')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_sequential_basic>
          nextitem: <Function test_run_with_progress_callback>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic
            location: ('tests/test_core.py', 110, 'TestMonteCarloSimulation.test_run_sequential_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_sequential_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_sequential_basic>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_sequential_basic>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6480> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_basic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_sequential_basic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_sequential_basic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_basic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_sequential_basic>
            nextitem: <Function test_run_with_progress_callback>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_sequential_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_sequential_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_basic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_basic
            location: ('tests/test_core.py', 110, 'TestMonteCarloSimulation.test_run_sequential_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_with_progress_callback>
          nextitem: <Function test_run_with_custom_percentiles>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback
            location: ('tests/test_core.py', 122, 'TestMonteCarloSimulation.test_run_with_progress_callback')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_with_progress_callback>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_progress_callback>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_progress_callback>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c65d0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_progress_callback>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_with_progress_callback>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_with_progress_callback>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_progress_callback>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_with_progress_callback>
            nextitem: <Function test_run_with_custom_percentiles>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_progress_callback>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_progress_callback>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_progress_callback>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_progress_callback
            location: ('tests/test_core.py', 122, 'TestMonteCarloSimulation.test_run_with_progress_callback')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_with_custom_percentiles>
          nextitem: <Function test_run_with_stats_engine>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles
            location: ('tests/test_core.py', 139, 'TestMonteCarloSimulation.test_run_with_custom_percentiles')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_with_custom_percentiles>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_custom_percentiles>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_custom_percentiles>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6750> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_custom_percentiles>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_with_custom_percentiles>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_with_custom_percentiles>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_custom_percentiles>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_with_custom_percentiles>
            nextitem: <Function test_run_with_stats_engine>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_custom_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_custom_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_custom_percentiles>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_custom_percentiles
            location: ('tests/test_core.py', 139, 'TestMonteCarloSimulation.test_run_with_custom_percentiles')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_with_stats_engine>
          nextitem: <Function test_run_parallel_basic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine
            location: ('tests/test_core.py', 150, 'TestMonteCarloSimulation.test_run_with_stats_engine')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_with_stats_engine>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_stats_engine>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_stats_engine>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6d80> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_stats_engine>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_with_stats_engine>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_with_stats_engine>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_stats_engine>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_with_stats_engine>
            nextitem: <Function test_run_parallel_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_with_stats_engine>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_stats_engine>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_stats_engine>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_with_stats_engine
            location: ('tests/test_core.py', 150, 'TestMonteCarloSimulation.test_run_with_stats_engine')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_parallel_basic>
          nextitem: <Function test_run_sequential_vs_parallel_reproducibility>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic
            location: ('tests/test_core.py', 166, 'TestMonteCarloSimulation.test_run_parallel_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_parallel_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_parallel_basic>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_parallel_basic>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7140> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_parallel_basic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_parallel_basic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_parallel_basic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_parallel_basic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_parallel_basic>
            nextitem: <Function test_run_sequential_vs_parallel_reproducibility>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_parallel_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_parallel_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_parallel_basic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_parallel_basic
            location: ('tests/test_core.py', 166, 'TestMonteCarloSimulation.test_run_parallel_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_sequential_vs_parallel_reproducibility>
          nextitem: <Function test_run_invalid_n_simulations>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility
            location: ('tests/test_core.py', 178, 'TestMonteCarloSimulation.test_run_sequential_vs_parallel_reproducibility')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_sequential_vs_parallel_reproducibility>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_sequential_vs_parallel_reproducibility>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6180> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_sequential_vs_parallel_reproducibility>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
            nextitem: <Function test_run_invalid_n_simulations>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_sequential_vs_parallel_reproducibility>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_sequential_vs_parallel_reproducibility>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_sequential_vs_parallel_reproducibility>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_sequential_vs_parallel_reproducibility
            location: ('tests/test_core.py', 178, 'TestMonteCarloSimulation.test_run_sequential_vs_parallel_reproducibility')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_invalid_n_simulations>
          nextitem: <Function test_serialization>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations
            location: ('tests/test_core.py', 189, 'TestMonteCarloSimulation.test_run_invalid_n_simulations')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_invalid_n_simulations>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_invalid_n_simulations>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_invalid_n_simulations>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6720> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_invalid_n_simulations>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_invalid_n_simulations>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_invalid_n_simulations>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_invalid_n_simulations>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_invalid_n_simulations>
            nextitem: <Function test_serialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_invalid_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_invalid_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_invalid_n_simulations>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_run_invalid_n_simulations
            location: ('tests/test_core.py', 189, 'TestMonteCarloSimulation.test_run_invalid_n_simulations')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_serialization>
          nextitem: <Function test_deterministic_results>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_serialization
            location: ('tests/test_core.py', 194, 'TestMonteCarloSimulation.test_serialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_serialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_serialization>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_serialization>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7170> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_serialization>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_serialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_serialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_serialization>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_serialization>
            nextitem: <Function test_deterministic_results>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_serialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_serialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_serialization>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_serialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_serialization
            location: ('tests/test_core.py', 194, 'TestMonteCarloSimulation.test_serialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_deterministic_results>
          nextitem: <Function test_framework_initialization>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results
            location: ('tests/test_core.py', 209, 'TestMonteCarloSimulation.test_deterministic_results')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_deterministic_results>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_deterministic_results>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='deterministic_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'deterministic_simulation' for <Function test_deterministic_results>>
          finish pytest_fixture_setup --> <conftest.DeterministicSim object at 0x1076c6180> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_deterministic_results>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_deterministic_results>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_deterministic_results>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_deterministic_results>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_deterministic_results>
            nextitem: <Function test_framework_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='deterministic_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'deterministic_simulation' for <Function test_deterministic_results>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_deterministic_results>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_deterministic_results>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloSimulation::test_deterministic_results
            location: ('tests/test_core.py', 209, 'TestMonteCarloSimulation.test_deterministic_results')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_framework_initialization>
          nextitem: <Function test_register_simulation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_framework_initialization
            location: ('tests/test_core.py', 219, 'TestMonteCarloFramework.test_framework_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_framework_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_framework_initialization>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_framework_initialization>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c6bd0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_initialization>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_framework_initialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_framework_initialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_initialization>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_framework_initialization>
            nextitem: <Function test_register_simulation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_framework_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_framework_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_initialization>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_framework_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_framework_initialization
            location: ('tests/test_core.py', 219, 'TestMonteCarloFramework.test_framework_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_register_simulation>
          nextitem: <Function test_register_simulation_custom_name>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_register_simulation
            location: ('tests/test_core.py', 224, 'TestMonteCarloFramework.test_register_simulation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_register_simulation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_register_simulation>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_register_simulation>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c7140> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_register_simulation>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6600> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_register_simulation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_register_simulation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_register_simulation>
            nextitem: <Function test_register_simulation_custom_name>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_register_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_register_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_register_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_register_simulation
            location: ('tests/test_core.py', 224, 'TestMonteCarloFramework.test_register_simulation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_register_simulation_custom_name>
          nextitem: <Function test_run_simulation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name
            location: ('tests/test_core.py', 229, 'TestMonteCarloFramework.test_register_simulation_custom_name')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_register_simulation_custom_name>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c6630> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7170> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation_custom_name>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_register_simulation_custom_name>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_register_simulation_custom_name>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation_custom_name>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_register_simulation_custom_name>
            nextitem: <Function test_run_simulation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_register_simulation_custom_name>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_register_simulation_custom_name>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_register_simulation_custom_name
            location: ('tests/test_core.py', 229, 'TestMonteCarloFramework.test_register_simulation_custom_name')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_simulation>
          nextitem: <Function test_run_simulation_not_found>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_run_simulation
            location: ('tests/test_core.py', 234, 'TestMonteCarloFramework.test_run_simulation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_simulation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_simulation>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_run_simulation>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c6d80> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_simulation>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c6bd0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_simulation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_simulation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_simulation>
            nextitem: <Function test_run_simulation_not_found>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_run_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_run_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_run_simulation
            location: ('tests/test_core.py', 234, 'TestMonteCarloFramework.test_run_simulation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_simulation_not_found>
          nextitem: <Function test_compare_results_mean>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found
            location: ('tests/test_core.py', 241, 'TestMonteCarloFramework.test_run_simulation_not_found')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_simulation_not_found>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_simulation_not_found>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_run_simulation_not_found>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c4dd0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation_not_found>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_simulation_not_found>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_simulation_not_found>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation_not_found>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_simulation_not_found>
            nextitem: <Function test_compare_results_mean>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_run_simulation_not_found>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_simulation_not_found>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_simulation_not_found>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_run_simulation_not_found
            location: ('tests/test_core.py', 241, 'TestMonteCarloFramework.test_run_simulation_not_found')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_mean>
          nextitem: <Function test_compare_results_std>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean
            location: ('tests/test_core.py', 246, 'TestMonteCarloFramework.test_compare_results_mean')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_mean>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_mean>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_mean>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c5010> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_mean>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7860> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_mean>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_mean>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_mean>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_mean>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_mean>
            nextitem: <Function test_compare_results_std>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_mean>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_mean>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_mean
            location: ('tests/test_core.py', 246, 'TestMonteCarloFramework.test_compare_results_mean')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_std>
          nextitem: <Function test_compare_results_percentile>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_std
            location: ('tests/test_core.py', 262, 'TestMonteCarloFramework.test_compare_results_std')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_std>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_std>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_std>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c6600> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_std>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7a70> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_std>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_std>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_std>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_std>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_std>
            nextitem: <Function test_compare_results_percentile>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_std>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_std>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_std>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_std>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_std' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_std
            location: ('tests/test_core.py', 262, 'TestMonteCarloFramework.test_compare_results_std')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_percentile>
          nextitem: <Function test_compare_results_no_results>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile
            location: ('tests/test_core.py', 271, 'TestMonteCarloFramework.test_compare_results_percentile')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_percentile>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_percentile>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_percentile>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c7140> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_percentile>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7d40> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_percentile>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_percentile>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_percentile>
            nextitem: <Function test_compare_results_no_results>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_percentile>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_percentile>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_percentile>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile
            location: ('tests/test_core.py', 271, 'TestMonteCarloFramework.test_compare_results_percentile')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_no_results>
          nextitem: <Function test_compare_results_invalid_metric>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results
            location: ('tests/test_core.py', 279, 'TestMonteCarloFramework.test_compare_results_no_results')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_no_results>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_no_results>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_no_results>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c7170> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_no_results>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_no_results>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_no_results>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_no_results>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_no_results>
            nextitem: <Function test_compare_results_invalid_metric>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_no_results>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_no_results>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_no_results>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_no_results
            location: ('tests/test_core.py', 279, 'TestMonteCarloFramework.test_compare_results_no_results')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_invalid_metric>
          nextitem: <Function test_compare_results_percentile_not_in_percentiles_dict>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric
            location: ('tests/test_core.py', 284, 'TestMonteCarloFramework.test_compare_results_invalid_metric')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_invalid_metric>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1076c5fd0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c7a70> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_invalid_metric>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_invalid_metric>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_invalid_metric>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_invalid_metric>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_invalid_metric>
            nextitem: <Function test_compare_results_percentile_not_in_percentiles_dict>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_invalid_metric>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_invalid_metric>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_invalid_metric
            location: ('tests/test_core.py', 284, 'TestMonteCarloFramework.test_compare_results_invalid_metric')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_percentile_not_in_percentiles_dict>
          nextitem: <Function test_stats_engine_percentiles_merge>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict
            location: ('tests/test_core.py', 292, 'TestMonteCarloFramework.test_compare_results_percentile_not_in_percentiles_dict')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_percentile_not_in_percentiles_dict>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_percentile_not_in_percentiles_dict>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
            nextitem: <Function test_stats_engine_percentiles_merge>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_percentile_not_in_percentiles_dict>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_percentile_not_in_percentiles_dict>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::test_compare_results_percentile_not_in_percentiles_dict
            location: ('tests/test_core.py', 292, 'TestMonteCarloFramework.test_compare_results_percentile_not_in_percentiles_dict')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_stats_engine_percentiles_merge>
          nextitem: <Function test_metadata_includes_requested_percentiles>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge
            location: ('tests/test_core.py', 326, 'TestMonteCarloFramework.TestPercentileMerging.test_stats_engine_percentiles_merge')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_stats_engine_percentiles_merge>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_percentiles_merge>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentiles_merge>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_stats_engine_percentiles_merge>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_stats_engine_percentiles_merge>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentiles_merge>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_stats_engine_percentiles_merge>
            nextitem: <Function test_metadata_includes_requested_percentiles>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_percentiles_merge>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentiles_merge>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMonteCarloFramework::TestPercentileMerging::test_stats_engine_percentiles_merge
            location: ('tests/test_core.py', 326, 'TestMonteCarloFramework.TestPercentileMerging.test_stats_engine_percentiles_merge')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_metadata_includes_requested_percentiles>
          nextitem: <Function test_metadata_includes_engine_defaults_used>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles
            location: ('tests/test_core.py', 349, 'TestMetadataFields.test_metadata_includes_requested_percentiles')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_metadata_includes_requested_percentiles>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_includes_requested_percentiles>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_requested_percentiles>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_metadata_includes_requested_percentiles>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_metadata_includes_requested_percentiles>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_requested_percentiles>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_metadata_includes_requested_percentiles>
            nextitem: <Function test_metadata_includes_engine_defaults_used>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_includes_requested_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_requested_percentiles>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_includes_requested_percentiles
            location: ('tests/test_core.py', 349, 'TestMetadataFields.test_metadata_includes_requested_percentiles')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_metadata_includes_engine_defaults_used>
          nextitem: <Function test_metadata_without_requested_percentiles>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used
            location: ('tests/test_core.py', 366, 'TestMetadataFields.test_metadata_includes_engine_defaults_used')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_includes_engine_defaults_used>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_metadata_includes_engine_defaults_used>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
            nextitem: <Function test_metadata_without_requested_percentiles>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_includes_engine_defaults_used>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_includes_engine_defaults_used>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_includes_engine_defaults_used
            location: ('tests/test_core.py', 366, 'TestMetadataFields.test_metadata_includes_engine_defaults_used')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_metadata_without_requested_percentiles>
          nextitem: <Function test_parallel_backend_thread_explicit>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles
            location: ('tests/test_core.py', 383, 'TestMetadataFields.test_metadata_without_requested_percentiles')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_metadata_without_requested_percentiles>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_without_requested_percentiles>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_without_requested_percentiles>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_metadata_without_requested_percentiles>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_metadata_without_requested_percentiles>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_without_requested_percentiles>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_metadata_without_requested_percentiles>
            nextitem: <Function test_parallel_backend_thread_explicit>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_metadata_without_requested_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_metadata_without_requested_percentiles>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestMetadataFields::test_metadata_without_requested_percentiles
            location: ('tests/test_core.py', 383, 'TestMetadataFields.test_metadata_without_requested_percentiles')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_backend_thread_explicit>
          nextitem: <Function test_parallel_backend_process_explicit>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit
            location: ('tests/test_core.py', 404, 'TestParallelBackend.test_parallel_backend_thread_explicit')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_backend_thread_explicit>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_thread_explicit>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_thread_explicit>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_backend_thread_explicit>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_backend_thread_explicit>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_thread_explicit>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_backend_thread_explicit>
            nextitem: <Function test_parallel_backend_process_explicit>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_thread_explicit>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_thread_explicit>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_thread_explicit
            location: ('tests/test_core.py', 404, 'TestParallelBackend.test_parallel_backend_thread_explicit')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_backend_process_explicit>
          nextitem: <Function test_parallel_backend_auto_uses_threads>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit
            location: ('tests/test_core.py', 420, 'TestParallelBackend.test_parallel_backend_process_explicit')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_backend_process_explicit>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_process_explicit>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_process_explicit>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_backend_process_explicit>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_backend_process_explicit>
          early skip of rewriting module: multiprocessing.synchronize [assertion]
          early skip of rewriting module: multiprocessing.resource_tracker [assertion]
          early skip of rewriting module: multiprocessing.spawn [assertion]
          early skip of rewriting module: runpy [assertion]
          early skip of rewriting module: _posixshmem [assertion]
          early skip of rewriting module: multiprocessing.popen_spawn_posix [assertion]
          early skip of rewriting module: multiprocessing.popen_fork [assertion]
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_process_explicit>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_backend_process_explicit>
            nextitem: <Function test_parallel_backend_auto_uses_threads>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_process_explicit>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_process_explicit>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_process_explicit
            location: ('tests/test_core.py', 420, 'TestParallelBackend.test_parallel_backend_process_explicit')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_backend_auto_uses_threads>
          nextitem: <Function test_parallel_fallback_small_n_simulations>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads
            location: ('tests/test_core.py', 436, 'TestParallelBackend.test_parallel_backend_auto_uses_threads')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_auto_uses_threads>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_backend_auto_uses_threads>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
            nextitem: <Function test_parallel_fallback_small_n_simulations>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_backend_auto_uses_threads>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_backend_auto_uses_threads>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestParallelBackend::test_parallel_backend_auto_uses_threads
            location: ('tests/test_core.py', 436, 'TestParallelBackend.test_parallel_backend_auto_uses_threads')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_fallback_small_n_simulations>
          nextitem: <Function test_parallel_fallback_single_worker>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations
            location: ('tests/test_core.py', 456, 'TestParallelFallback.test_parallel_fallback_small_n_simulations')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_fallback_small_n_simulations>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_fallback_small_n_simulations>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
            nextitem: <Function test_parallel_fallback_single_worker>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_fallback_small_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_small_n_simulations>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestParallelFallback::test_parallel_fallback_small_n_simulations
            location: ('tests/test_core.py', 456, 'TestParallelFallback.test_parallel_fallback_small_n_simulations')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_fallback_single_worker>
          nextitem: <Function test_default_backend_is_auto>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker
            location: ('tests/test_core.py', 473, 'TestParallelFallback.test_parallel_fallback_single_worker')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_fallback_single_worker>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_fallback_single_worker>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_single_worker>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_fallback_single_worker>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_fallback_single_worker>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_single_worker>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_fallback_single_worker>
            nextitem: <Function test_default_backend_is_auto>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_fallback_single_worker>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_fallback_single_worker>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestParallelFallback::test_parallel_fallback_single_worker
            location: ('tests/test_core.py', 473, 'TestParallelFallback.test_parallel_fallback_single_worker')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_default_backend_is_auto>
          nextitem: <Function test_thread_backend_with_large_job>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto
            location: ('tests/test_core.py', 492, 'TestThreadBackendExecution.test_default_backend_is_auto')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_default_backend_is_auto>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_backend_is_auto>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_backend_is_auto>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_default_backend_is_auto>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_default_backend_is_auto>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_backend_is_auto>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_default_backend_is_auto>
            nextitem: <Function test_thread_backend_with_large_job>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_backend_is_auto>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_backend_is_auto>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestThreadBackendExecution::test_default_backend_is_auto
            location: ('tests/test_core.py', 492, 'TestThreadBackendExecution.test_default_backend_is_auto')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_thread_backend_with_large_job>
          nextitem: <Function test_parallel_without_seed_generates_random_sequences>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job
            location: ('tests/test_core.py', 500, 'TestThreadBackendExecution.test_thread_backend_with_large_job')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_thread_backend_with_large_job>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_thread_backend_with_large_job>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_thread_backend_with_large_job>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_thread_backend_with_large_job>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_thread_backend_with_large_job>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_thread_backend_with_large_job>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_thread_backend_with_large_job>
            nextitem: <Function test_parallel_without_seed_generates_random_sequences>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_thread_backend_with_large_job>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_thread_backend_with_large_job>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestThreadBackendExecution::test_thread_backend_with_large_job
            location: ('tests/test_core.py', 500, 'TestThreadBackendExecution.test_thread_backend_with_large_job')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_without_seed_generates_random_sequences>
          nextitem: <Function test_keyboard_interrupt_cleanup>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences
            location: ('tests/test_core.py', 520, 'TestSeedSequenceGeneration.test_parallel_without_seed_generates_random_sequences')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_without_seed_generates_random_sequences>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_without_seed_generates_random_sequences>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
            nextitem: <Function test_keyboard_interrupt_cleanup>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_without_seed_generates_random_sequences>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_without_seed_generates_random_sequences>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestSeedSequenceGeneration::test_parallel_without_seed_generates_random_sequences
            location: ('tests/test_core.py', 520, 'TestSeedSequenceGeneration.test_parallel_without_seed_generates_random_sequences')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_keyboard_interrupt_cleanup>
          nextitem: <Function test_run_without_percentiles_and_without_stats>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup
            location: ('tests/test_core.py', 541, 'TestKeyboardInterruptHandling.test_keyboard_interrupt_cleanup')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_keyboard_interrupt_cleanup>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_keyboard_interrupt_cleanup>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_keyboard_interrupt_cleanup>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_keyboard_interrupt_cleanup>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_keyboard_interrupt_cleanup>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_keyboard_interrupt_cleanup>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_keyboard_interrupt_cleanup>
            nextitem: <Function test_run_without_percentiles_and_without_stats>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_keyboard_interrupt_cleanup>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_keyboard_interrupt_cleanup>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestKeyboardInterruptHandling::test_keyboard_interrupt_cleanup
            location: ('tests/test_core.py', 541, 'TestKeyboardInterruptHandling.test_keyboard_interrupt_cleanup')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_without_percentiles_and_without_stats>
          nextitem: <Function test_run_with_empty_percentiles_list>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats
            location: ('tests/test_core.py', 573, 'TestAdditionalEdgeCases.test_run_without_percentiles_and_without_stats')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_without_percentiles_and_without_stats>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_without_percentiles_and_without_stats>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
            nextitem: <Function test_run_with_empty_percentiles_list>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_without_percentiles_and_without_stats>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_without_percentiles_and_without_stats>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_run_without_percentiles_and_without_stats
            location: ('tests/test_core.py', 573, 'TestAdditionalEdgeCases.test_run_without_percentiles_and_without_stats')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_with_empty_percentiles_list>
          nextitem: <Function test_parallel_with_custom_block_size>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list
            location: ('tests/test_core.py', 590, 'TestAdditionalEdgeCases.test_run_with_empty_percentiles_list')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_with_empty_percentiles_list>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_empty_percentiles_list>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_empty_percentiles_list>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_with_empty_percentiles_list>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_with_empty_percentiles_list>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_empty_percentiles_list>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_with_empty_percentiles_list>
            nextitem: <Function test_parallel_with_custom_block_size>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_run_with_empty_percentiles_list>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_with_empty_percentiles_list>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_run_with_empty_percentiles_list
            location: ('tests/test_core.py', 590, 'TestAdditionalEdgeCases.test_run_with_empty_percentiles_list')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_with_custom_block_size>
          nextitem: <Function test_compare_results_with_all_metrics>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size
            location: ('tests/test_core.py', 606, 'TestAdditionalEdgeCases.test_parallel_with_custom_block_size')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_with_custom_block_size>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_with_custom_block_size>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_with_custom_block_size>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_with_custom_block_size>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_with_custom_block_size>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_with_custom_block_size>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_with_custom_block_size>
            nextitem: <Function test_compare_results_with_all_metrics>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_with_custom_block_size>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_with_custom_block_size>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_parallel_with_custom_block_size
            location: ('tests/test_core.py', 606, 'TestAdditionalEdgeCases.test_parallel_with_custom_block_size')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_compare_results_with_all_metrics>
          nextitem: <Function test_chebyshev_required_n>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics
            location: ('tests/test_core.py', 621, 'TestAdditionalEdgeCases.test_compare_results_with_all_metrics')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_compare_results_with_all_metrics>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_with_all_metrics>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_with_all_metrics>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_compare_results_with_all_metrics>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_compare_results_with_all_metrics>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_with_all_metrics>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_compare_results_with_all_metrics>
            nextitem: <Function test_chebyshev_required_n>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_compare_results_with_all_metrics>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_compare_results_with_all_metrics>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_core.py::TestAdditionalEdgeCases::test_compare_results_with_all_metrics
            location: ('tests/test_core.py', 621, 'TestAdditionalEdgeCases.test_compare_results_with_all_metrics')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_chebyshev_required_n>
          nextitem: <Function test_chebyshev_required_n_no_eps>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n
            location: ('tests/test_distribution_free_metrics.py', 14, 'TestDistributionFreeMetrics.test_chebyshev_required_n')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_chebyshev_required_n>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_chebyshev_required_n>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_chebyshev_required_n>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_chebyshev_required_n>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_chebyshev_required_n>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_chebyshev_required_n>
            nextitem: <Function test_chebyshev_required_n_no_eps>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_chebyshev_required_n>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_chebyshev_required_n>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n
            location: ('tests/test_distribution_free_metrics.py', 14, 'TestDistributionFreeMetrics.test_chebyshev_required_n')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_chebyshev_required_n_no_eps>
          nextitem: <Function test_markov_error_prob>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps
            location: ('tests/test_distribution_free_metrics.py', 22, 'TestDistributionFreeMetrics.test_chebyshev_required_n_no_eps')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_chebyshev_required_n_no_eps>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_chebyshev_required_n_no_eps>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_chebyshev_required_n_no_eps>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n_no_eps>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_chebyshev_required_n_no_eps>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_chebyshev_required_n_no_eps>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n_no_eps>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_chebyshev_required_n_no_eps>
            nextitem: <Function test_markov_error_prob>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_chebyshev_required_n_no_eps>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_chebyshev_required_n_no_eps>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_chebyshev_required_n_no_eps>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_chebyshev_required_n_no_eps
            location: ('tests/test_distribution_free_metrics.py', 22, 'TestDistributionFreeMetrics.test_chebyshev_required_n_no_eps')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_markov_error_prob>
          nextitem: <Function test_markov_error_prob_no_target>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob
            location: ('tests/test_distribution_free_metrics.py', 28, 'TestDistributionFreeMetrics.test_markov_error_prob')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_markov_error_prob>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_markov_error_prob>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_markov_error_prob>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_markov_error_prob>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_markov_error_prob>
            nextitem: <Function test_markov_error_prob_no_target>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_markov_error_prob>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob
            location: ('tests/test_distribution_free_metrics.py', 28, 'TestDistributionFreeMetrics.test_markov_error_prob')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_markov_error_prob_no_target>
          nextitem: <Function test_bias_to_target>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target
            location: ('tests/test_distribution_free_metrics.py', 37, 'TestDistributionFreeMetrics.test_markov_error_prob_no_target')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_markov_error_prob_no_target>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_markov_error_prob_no_target>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_markov_error_prob_no_target>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob_no_target>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_markov_error_prob_no_target>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_markov_error_prob_no_target>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob_no_target>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_markov_error_prob_no_target>
            nextitem: <Function test_bias_to_target>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_markov_error_prob_no_target>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_markov_error_prob_no_target>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_markov_error_prob_no_target>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_markov_error_prob_no_target
            location: ('tests/test_distribution_free_metrics.py', 37, 'TestDistributionFreeMetrics.test_markov_error_prob_no_target')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_bias_to_target>
          nextitem: <Function test_mse_to_target>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target
            location: ('tests/test_distribution_free_metrics.py', 43, 'TestDistributionFreeMetrics.test_bias_to_target')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_bias_to_target>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_bias_to_target>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_bias_to_target>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_bias_to_target>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_bias_to_target>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_bias_to_target>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_bias_to_target>
            nextitem: <Function test_mse_to_target>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_bias_to_target>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_bias_to_target>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_bias_to_target
            location: ('tests/test_distribution_free_metrics.py', 43, 'TestDistributionFreeMetrics.test_bias_to_target')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_mse_to_target>
          nextitem: <Function test_single_simulation_run>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target
            location: ('tests/test_distribution_free_metrics.py', 51, 'TestDistributionFreeMetrics.test_mse_to_target')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_mse_to_target>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mse_to_target>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mse_to_target>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_mse_to_target>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_mse_to_target>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mse_to_target>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_mse_to_target>
            nextitem: <Function test_single_simulation_run>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_mse_to_target>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_mse_to_target>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_distribution_free_metrics.py::TestDistributionFreeMetrics::test_mse_to_target
            location: ('tests/test_distribution_free_metrics.py', 51, 'TestDistributionFreeMetrics.test_mse_to_target')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_single_simulation_run>
          nextitem: <Function test_very_small_confidence_interval>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run
            location: ('tests/test_errors_and_edge_cases.py', 12, 'TestEdgeCases.test_single_simulation_run')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_single_simulation_run>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_single_simulation_run>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_single_simulation_run>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1076c4ec0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_single_simulation_run>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_single_simulation_run>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_single_simulation_run>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_single_simulation_run>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_single_simulation_run>
            nextitem: <Function test_very_small_confidence_interval>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_single_simulation_run>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_single_simulation_run>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_single_simulation_run>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_single_simulation_run
            location: ('tests/test_errors_and_edge_cases.py', 12, 'TestEdgeCases.test_single_simulation_run')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_very_small_confidence_interval>
          nextitem: <Function test_very_high_confidence_interval>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval
            location: ('tests/test_errors_and_edge_cases.py', 19, 'TestEdgeCases.test_very_small_confidence_interval')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_very_small_confidence_interval>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_very_small_confidence_interval>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_small_confidence_interval>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_very_small_confidence_interval>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_very_small_confidence_interval>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_small_confidence_interval>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_very_small_confidence_interval>
            nextitem: <Function test_very_high_confidence_interval>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_very_small_confidence_interval>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_small_confidence_interval>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_small_confidence_interval
            location: ('tests/test_errors_and_edge_cases.py', 19, 'TestEdgeCases.test_very_small_confidence_interval')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_very_high_confidence_interval>
          nextitem: <Function test_negative_portfolio_values_possible>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval
            location: ('tests/test_errors_and_edge_cases.py', 27, 'TestEdgeCases.test_very_high_confidence_interval')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_very_high_confidence_interval>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_very_high_confidence_interval>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_high_confidence_interval>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_very_high_confidence_interval>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_very_high_confidence_interval>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_high_confidence_interval>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_very_high_confidence_interval>
            nextitem: <Function test_negative_portfolio_values_possible>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_very_high_confidence_interval>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_very_high_confidence_interval>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_very_high_confidence_interval
            location: ('tests/test_errors_and_edge_cases.py', 27, 'TestEdgeCases.test_very_high_confidence_interval')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_negative_portfolio_values_possible>
          nextitem: <Function test_zero_year_portfolio>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible
            location: ('tests/test_errors_and_edge_cases.py', 35, 'TestEdgeCases.test_negative_portfolio_values_possible')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_negative_portfolio_values_possible>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_negative_portfolio_values_possible>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_portfolio_values_possible>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_negative_portfolio_values_possible>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_negative_portfolio_values_possible>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_portfolio_values_possible>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_negative_portfolio_values_possible>
            nextitem: <Function test_zero_year_portfolio>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_negative_portfolio_values_possible>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_portfolio_values_possible>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_negative_portfolio_values_possible
            location: ('tests/test_errors_and_edge_cases.py', 35, 'TestEdgeCases.test_negative_portfolio_values_possible')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_zero_year_portfolio>
          nextitem: <Function test_nan_handling_in_stats>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio
            location: ('tests/test_errors_and_edge_cases.py', 52, 'TestEdgeCases.test_zero_year_portfolio')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_zero_year_portfolio>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_zero_year_portfolio>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_year_portfolio>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_zero_year_portfolio>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_zero_year_portfolio>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_year_portfolio>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_zero_year_portfolio>
            nextitem: <Function test_nan_handling_in_stats>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_zero_year_portfolio>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_year_portfolio>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_zero_year_portfolio
            location: ('tests/test_errors_and_edge_cases.py', 52, 'TestEdgeCases.test_zero_year_portfolio')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_nan_handling_in_stats>
          nextitem: <Function test_empty_percentiles_list>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats
            location: ('tests/test_errors_and_edge_cases.py', 65, 'TestEdgeCases.test_nan_handling_in_stats')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_nan_handling_in_stats>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_nan_handling_in_stats>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_nan_handling_in_stats>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_nan_handling_in_stats>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_nan_handling_in_stats>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_nan_handling_in_stats>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_nan_handling_in_stats>
            nextitem: <Function test_empty_percentiles_list>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_nan_handling_in_stats>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_nan_handling_in_stats>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_nan_handling_in_stats
            location: ('tests/test_errors_and_edge_cases.py', 65, 'TestEdgeCases.test_nan_handling_in_stats')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_empty_percentiles_list>
          nextitem: <Function test_duplicate_percentiles>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list
            location: ('tests/test_errors_and_edge_cases.py', 74, 'TestEdgeCases.test_empty_percentiles_list')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_empty_percentiles_list>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_empty_percentiles_list>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_empty_percentiles_list>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107971970> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_empty_percentiles_list>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_empty_percentiles_list>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_empty_percentiles_list>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_empty_percentiles_list>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_empty_percentiles_list>
            nextitem: <Function test_duplicate_percentiles>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_empty_percentiles_list>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_empty_percentiles_list>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_empty_percentiles_list>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_empty_percentiles_list
            location: ('tests/test_errors_and_edge_cases.py', 74, 'TestEdgeCases.test_empty_percentiles_list')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_duplicate_percentiles>
          nextitem: <Function test_invalid_simulation_name_in_framework>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles
            location: ('tests/test_errors_and_edge_cases.py', 85, 'TestEdgeCases.test_duplicate_percentiles')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_duplicate_percentiles>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_duplicate_percentiles>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_duplicate_percentiles>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107970ef0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_duplicate_percentiles>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_duplicate_percentiles>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_duplicate_percentiles>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_duplicate_percentiles>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_duplicate_percentiles>
            nextitem: <Function test_invalid_simulation_name_in_framework>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_duplicate_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_duplicate_percentiles>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_duplicate_percentiles>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestEdgeCases::test_duplicate_percentiles
            location: ('tests/test_errors_and_edge_cases.py', 85, 'TestEdgeCases.test_duplicate_percentiles')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_invalid_simulation_name_in_framework>
          nextitem: <Function test_negative_n_simulations>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework
            location: ('tests/test_errors_and_edge_cases.py', 100, 'TestErrorHandling.test_invalid_simulation_name_in_framework')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_invalid_simulation_name_in_framework>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_invalid_simulation_name_in_framework>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_invalid_simulation_name_in_framework>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x107970ef0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_simulation_name_in_framework>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_invalid_simulation_name_in_framework>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_invalid_simulation_name_in_framework>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_simulation_name_in_framework>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_invalid_simulation_name_in_framework>
            nextitem: <Function test_negative_n_simulations>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_invalid_simulation_name_in_framework>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_invalid_simulation_name_in_framework>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_simulation_name_in_framework>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_simulation_name_in_framework
            location: ('tests/test_errors_and_edge_cases.py', 100, 'TestErrorHandling.test_invalid_simulation_name_in_framework')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_negative_n_simulations>
          nextitem: <Function test_zero_n_simulations>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations
            location: ('tests/test_errors_and_edge_cases.py', 105, 'TestErrorHandling.test_negative_n_simulations')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_negative_n_simulations>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_negative_n_simulations>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_negative_n_simulations>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107970ef0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_n_simulations>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_negative_n_simulations>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_negative_n_simulations>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_n_simulations>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_negative_n_simulations>
            nextitem: <Function test_zero_n_simulations>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_negative_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_negative_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_negative_n_simulations>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_negative_n_simulations
            location: ('tests/test_errors_and_edge_cases.py', 105, 'TestErrorHandling.test_negative_n_simulations')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_zero_n_simulations>
          nextitem: <Function test_invalid_percentile_metric_comparison>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations
            location: ('tests/test_errors_and_edge_cases.py', 110, 'TestErrorHandling.test_zero_n_simulations')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_zero_n_simulations>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_zero_n_simulations>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_zero_n_simulations>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107970ef0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_n_simulations>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_zero_n_simulations>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_zero_n_simulations>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_n_simulations>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_zero_n_simulations>
            nextitem: <Function test_invalid_percentile_metric_comparison>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_zero_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_zero_n_simulations>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_zero_n_simulations>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_zero_n_simulations
            location: ('tests/test_errors_and_edge_cases.py', 110, 'TestErrorHandling.test_zero_n_simulations')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_invalid_percentile_metric_comparison>
          nextitem: <Function test_stats_engine_with_empty_metrics>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison
            location: ('tests/test_errors_and_edge_cases.py', 115, 'TestErrorHandling.test_invalid_percentile_metric_comparison')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_invalid_percentile_metric_comparison>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x1079719d0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107971070> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_percentile_metric_comparison>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_invalid_percentile_metric_comparison>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_invalid_percentile_metric_comparison>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_percentile_metric_comparison>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_invalid_percentile_metric_comparison>
            nextitem: <Function test_stats_engine_with_empty_metrics>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_invalid_percentile_metric_comparison>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_invalid_percentile_metric_comparison>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_invalid_percentile_metric_comparison
            location: ('tests/test_errors_and_edge_cases.py', 115, 'TestErrorHandling.test_invalid_percentile_metric_comparison')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_stats_engine_with_empty_metrics>
          nextitem: <Function test_simulation_with_failed_stats>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics
            location: ('tests/test_errors_and_edge_cases.py', 123, 'TestErrorHandling.test_stats_engine_with_empty_metrics')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_stats_engine_with_empty_metrics>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_with_empty_metrics>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_with_empty_metrics>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_stats_engine_with_empty_metrics>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_stats_engine_with_empty_metrics>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_with_empty_metrics>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_stats_engine_with_empty_metrics>
            nextitem: <Function test_simulation_with_failed_stats>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_with_empty_metrics>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_with_empty_metrics>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_stats_engine_with_empty_metrics
            location: ('tests/test_errors_and_edge_cases.py', 123, 'TestErrorHandling.test_stats_engine_with_empty_metrics')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simulation_with_failed_stats>
          nextitem: <Function test_end_to_end_pi_estimation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats
            location: ('tests/test_errors_and_edge_cases.py', 129, 'TestErrorHandling.test_simulation_with_failed_stats')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simulation_with_failed_stats>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1079711f0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='monkeypatch' scope='function' baseid=''>
              request: <SubRequest 'monkeypatch' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_setup --> <_pytest.monkeypatch.MonkeyPatch object at 0x107971100> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_failed_stats>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simulation_with_failed_stats>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simulation_with_failed_stats>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_failed_stats>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simulation_with_failed_stats>
            nextitem: <Function test_end_to_end_pi_estimation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='monkeypatch' scope='function' baseid=''>
              request: <SubRequest 'monkeypatch' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_with_failed_stats>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_failed_stats>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_errors_and_edge_cases.py::TestErrorHandling::test_simulation_with_failed_stats
            location: ('tests/test_errors_and_edge_cases.py', 129, 'TestErrorHandling.test_simulation_with_failed_stats')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_end_to_end_pi_estimation>
          nextitem: <Function test_end_to_end_portfolio>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation
            location: ('tests/test_integration.py', 10, 'TestIntegration.test_end_to_end_pi_estimation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_end_to_end_pi_estimation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_end_to_end_pi_estimation>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_pi_estimation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_end_to_end_pi_estimation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_end_to_end_pi_estimation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_pi_estimation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_end_to_end_pi_estimation>
            nextitem: <Function test_end_to_end_portfolio>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_end_to_end_pi_estimation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_pi_estimation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_end_to_end_pi_estimation
            location: ('tests/test_integration.py', 10, 'TestIntegration.test_end_to_end_pi_estimation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_end_to_end_portfolio>
          nextitem: <Function test_multiple_simulations_comparison>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_end_to_end_portfolio
            location: ('tests/test_integration.py', 36, 'TestIntegration.test_end_to_end_portfolio')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_end_to_end_portfolio>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_end_to_end_portfolio>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_portfolio>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_end_to_end_portfolio>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_end_to_end_portfolio>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_portfolio>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_end_to_end_portfolio>
            nextitem: <Function test_multiple_simulations_comparison>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_end_to_end_portfolio>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_end_to_end_portfolio>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_end_to_end_portfolio' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_end_to_end_portfolio
            location: ('tests/test_integration.py', 36, 'TestIntegration.test_end_to_end_portfolio')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_multiple_simulations_comparison>
          nextitem: <Function test_parallel_execution_consistency>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison
            location: ('tests/test_integration.py', 61, 'TestIntegration.test_multiple_simulations_comparison')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_multiple_simulations_comparison>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_multiple_simulations_comparison>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_multiple_simulations_comparison>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_multiple_simulations_comparison>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_multiple_simulations_comparison>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_multiple_simulations_comparison>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_multiple_simulations_comparison>
            nextitem: <Function test_parallel_execution_consistency>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_multiple_simulations_comparison>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_multiple_simulations_comparison>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_multiple_simulations_comparison
            location: ('tests/test_integration.py', 61, 'TestIntegration.test_multiple_simulations_comparison')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_execution_consistency>
          nextitem: <Function test_large_scale_simulation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_parallel_execution_consistency
            location: ('tests/test_integration.py', 89, 'TestIntegration.test_parallel_execution_consistency')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_execution_consistency>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_execution_consistency>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_execution_consistency>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_execution_consistency>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_execution_consistency>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_execution_consistency>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_execution_consistency>
            nextitem: <Function test_large_scale_simulation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_execution_consistency>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_execution_consistency>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_parallel_execution_consistency' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_parallel_execution_consistency
            location: ('tests/test_integration.py', 89, 'TestIntegration.test_parallel_execution_consistency')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_large_scale_simulation>
          nextitem: <Function test_custom_simulation_integration>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_large_scale_simulation
            location: ('tests/test_integration.py', 103, 'TestIntegration.test_large_scale_simulation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_large_scale_simulation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_large_scale_simulation>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_large_scale_simulation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_large_scale_simulation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_large_scale_simulation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_large_scale_simulation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_large_scale_simulation>
            nextitem: <Function test_custom_simulation_integration>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_large_scale_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_large_scale_simulation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_large_scale_simulation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_large_scale_simulation
            location: ('tests/test_integration.py', 103, 'TestIntegration.test_large_scale_simulation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_custom_simulation_integration>
          nextitem: <Function test_simulation_kwargs_passthrough>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_custom_simulation_integration
            location: ('tests/test_integration.py', 126, 'TestIntegration.test_custom_simulation_integration')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_custom_simulation_integration>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_custom_simulation_integration>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_custom_simulation_integration>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_custom_simulation_integration>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_custom_simulation_integration>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_custom_simulation_integration>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_custom_simulation_integration>
            nextitem: <Function test_simulation_kwargs_passthrough>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_custom_simulation_integration>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_custom_simulation_integration>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_custom_simulation_integration' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_custom_simulation_integration
            location: ('tests/test_integration.py', 126, 'TestIntegration.test_custom_simulation_integration')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simulation_kwargs_passthrough>
          nextitem: <Function test_simulation_with_mocked_rng>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough
            location: ('tests/test_integration.py', 143, 'TestIntegration.test_simulation_kwargs_passthrough')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simulation_kwargs_passthrough>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_kwargs_passthrough>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_kwargs_passthrough>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simulation_kwargs_passthrough>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simulation_kwargs_passthrough>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_kwargs_passthrough>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simulation_kwargs_passthrough>
            nextitem: <Function test_simulation_with_mocked_rng>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_kwargs_passthrough>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_kwargs_passthrough>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_integration.py::TestIntegration::test_simulation_kwargs_passthrough
            location: ('tests/test_integration.py', 143, 'TestIntegration.test_simulation_kwargs_passthrough')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_simulation_with_mocked_rng>
          nextitem: <Function test_framework_with_mocked_simulation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng
            location: ('tests/test_mocking_and_isolation.py', 10, 'TestMocking.test_simulation_with_mocked_rng')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_simulation_with_mocked_rng>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_with_mocked_rng>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_mocked_rng>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_simulation_with_mocked_rng>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_simulation_with_mocked_rng>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_mocked_rng>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_simulation_with_mocked_rng>
            nextitem: <Function test_framework_with_mocked_simulation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_simulation_with_mocked_rng>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_simulation_with_mocked_rng>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_simulation_with_mocked_rng
            location: ('tests/test_mocking_and_isolation.py', 10, 'TestMocking.test_simulation_with_mocked_rng')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_framework_with_mocked_simulation>
          nextitem: <Function test_progress_callback_mock>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation
            location: ('tests/test_mocking_and_isolation.py', 27, 'TestMocking.test_framework_with_mocked_simulation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_framework_with_mocked_simulation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_framework_with_mocked_simulation>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_framework_with_mocked_simulation>>
          finish pytest_fixture_setup --> <mcframework.core.MonteCarloFramework object at 0x10796a120> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_with_mocked_simulation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_framework_with_mocked_simulation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_framework_with_mocked_simulation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_with_mocked_simulation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_framework_with_mocked_simulation>
            nextitem: <Function test_progress_callback_mock>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='framework' scope='function' baseid='tests'>
              request: <SubRequest 'framework' for <Function test_framework_with_mocked_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_framework_with_mocked_simulation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_framework_with_mocked_simulation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_framework_with_mocked_simulation
            location: ('tests/test_mocking_and_isolation.py', 27, 'TestMocking.test_framework_with_mocked_simulation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_progress_callback_mock>
          nextitem: <Function test_various_simulation_sizes[1]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock
            location: ('tests/test_mocking_and_isolation.py', 48, 'TestMocking.test_progress_callback_mock')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_progress_callback_mock>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_progress_callback_mock>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_progress_callback_mock>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107968410> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_progress_callback_mock>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_progress_callback_mock>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_progress_callback_mock>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_progress_callback_mock>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_progress_callback_mock>
            nextitem: <Function test_various_simulation_sizes[1]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_progress_callback_mock>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_progress_callback_mock>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_progress_callback_mock>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_mocking_and_isolation.py::TestMocking::test_progress_callback_mock
            location: ('tests/test_mocking_and_isolation.py', 48, 'TestMocking.test_progress_callback_mock')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_simulation_sizes[1]>
          nextitem: <Function test_various_simulation_sizes[10]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[1]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_simulation_sizes[1]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107968410> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_setup --> 1 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_simulation_sizes[1]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_simulation_sizes[1]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_simulation_sizes[1]>
            nextitem: <Function test_various_simulation_sizes[10]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[1]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_simulation_sizes[10]>
          nextitem: <Function test_various_simulation_sizes[100]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[10]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_simulation_sizes[10]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107969130> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_setup --> 10 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[10]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_simulation_sizes[10]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_simulation_sizes[10]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[10]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_simulation_sizes[10]>
            nextitem: <Function test_various_simulation_sizes[100]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[10]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[10]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[10]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[10]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_simulation_sizes[100]>
          nextitem: <Function test_various_simulation_sizes[1000]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[100]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_simulation_sizes[100]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1079686b0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_setup --> 100 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[100]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_simulation_sizes[100]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_simulation_sizes[100]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[100]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_simulation_sizes[100]>
            nextitem: <Function test_various_simulation_sizes[1000]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[100]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[100]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[100]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[100]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_simulation_sizes[1000]>
          nextitem: <Function test_various_confidence_levels[0.9]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[1000]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_simulation_sizes[1000]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107968ec0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_setup --> 1000 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1000]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_simulation_sizes[1000]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_simulation_sizes[1000]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1000]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_simulation_sizes[1000]>
            nextitem: <Function test_various_confidence_levels[0.9]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_sims' scope='function' baseid=''>
              request: <SubRequest 'n_sims' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_simulation_sizes[1000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_simulation_sizes[1000]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_simulation_sizes[1000]
            location: ('tests/test_parametrized.py', 9, 'TestParametrized.test_various_simulation_sizes[1000]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_confidence_levels[0.9]>
          nextitem: <Function test_various_confidence_levels[0.95]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.9]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_confidence_levels[0.9]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_setup --> 0.9 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.9]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_confidence_levels[0.9]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_confidence_levels[0.9]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.9]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_confidence_levels[0.9]>
            nextitem: <Function test_various_confidence_levels[0.95]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.9]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.9]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.9]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.9]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_confidence_levels[0.95]>
          nextitem: <Function test_various_confidence_levels[0.99]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.95]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_confidence_levels[0.95]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_setup --> 0.95 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.95]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_confidence_levels[0.95]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_confidence_levels[0.95]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.95]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_confidence_levels[0.95]>
            nextitem: <Function test_various_confidence_levels[0.99]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.95]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.95]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.95]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.95]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_confidence_levels[0.99]>
          nextitem: <Function test_various_ci_methods[z]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.99]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_confidence_levels[0.99]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_setup --> 0.99 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.99]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_confidence_levels[0.99]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_confidence_levels[0.99]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.99]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_confidence_levels[0.99]>
            nextitem: <Function test_various_ci_methods[z]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='confidence' scope='function' baseid=''>
              request: <SubRequest 'confidence' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_confidence_levels[0.99]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_confidence_levels[0.99]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_confidence_levels[0.99]
            location: ('tests/test_parametrized.py', 16, 'TestParametrized.test_various_confidence_levels[0.99]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_ci_methods[z]>
          nextitem: <Function test_various_ci_methods[t]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[z]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_ci_methods[z]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_setup --> z [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[z]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_ci_methods[z]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_ci_methods[z]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[z]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_ci_methods[z]>
            nextitem: <Function test_various_ci_methods[t]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[z]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[z]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[z]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[z]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_ci_methods[t]>
          nextitem: <Function test_various_ci_methods[auto]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[t]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_ci_methods[t]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_setup --> t [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[t]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_ci_methods[t]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_ci_methods[t]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[t]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_ci_methods[t]>
            nextitem: <Function test_various_ci_methods[auto]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[t]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[t]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[t]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[t]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_ci_methods[auto]>
          nextitem: <Function test_various_worker_counts[1]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[auto]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_ci_methods[auto]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_setup --> auto [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[auto]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_ci_methods[auto]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_ci_methods[auto]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[auto]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_ci_methods[auto]>
            nextitem: <Function test_various_worker_counts[1]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='method' scope='function' baseid=''>
              request: <SubRequest 'method' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_ci_methods[auto]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_ci_methods[auto]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_ci_methods[auto]
            location: ('tests/test_parametrized.py', 23, 'TestParametrized.test_various_ci_methods[auto]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_worker_counts[1]>
          nextitem: <Function test_various_worker_counts[2]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[1]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_worker_counts[1]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1079710d0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_setup --> 1 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[1]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_worker_counts[1]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_worker_counts[1]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[1]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_worker_counts[1]>
            nextitem: <Function test_various_worker_counts[2]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[1]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[1]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[1]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[1]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_worker_counts[2]>
          nextitem: <Function test_various_worker_counts[4]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[2]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_worker_counts[2]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107971370> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_setup --> 2 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[2]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_worker_counts[2]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_worker_counts[2]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[2]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_worker_counts[2]>
            nextitem: <Function test_various_worker_counts[4]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[2]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[2]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[2]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[2]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_worker_counts[4]>
          nextitem: <Function test_various_block_sizes[100]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[4]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_worker_counts[4]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107970fe0> [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_setup --> 4 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[4]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_worker_counts[4]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_worker_counts[4]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[4]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_worker_counts[4]>
            nextitem: <Function test_various_block_sizes[100]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='n_workers' scope='function' baseid=''>
              request: <SubRequest 'n_workers' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_worker_counts[4]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_worker_counts[4]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_worker_counts[4]
            location: ('tests/test_parametrized.py', 30, 'TestParametrized.test_various_worker_counts[4]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_block_sizes[100]>
          nextitem: <Function test_various_block_sizes[1000]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[100]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_block_sizes[100]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[100]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[100]>>
          finish pytest_fixture_setup --> 100 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[100]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_block_sizes[100]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_block_sizes[100]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[100]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_block_sizes[100]>
            nextitem: <Function test_various_block_sizes[1000]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[100]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[100]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[100]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[100]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[100]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_block_sizes[1000]>
          nextitem: <Function test_various_block_sizes[10000]>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[1000]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_block_sizes[1000]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[1000]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[1000]>>
          finish pytest_fixture_setup --> 1000 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[1000]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_block_sizes[1000]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_block_sizes[1000]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[1000]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_block_sizes[1000]>
            nextitem: <Function test_various_block_sizes[10000]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[1000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[1000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[1000]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[1000]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[1000]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_various_block_sizes[10000]>
          nextitem: <Function test_parallel_faster_than_sequential>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[10000]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_various_block_sizes[10000]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[10000]>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[10000]>>
          finish pytest_fixture_setup --> 10000 [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[10000]>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_various_block_sizes[10000]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_various_block_sizes[10000]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[10000]>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_various_block_sizes[10000]>
            nextitem: <Function test_parallel_faster_than_sequential>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='block_size' scope='function' baseid=''>
              request: <SubRequest 'block_size' for <Function test_various_block_sizes[10000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_various_block_sizes[10000]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_various_block_sizes[10000]>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_parametrized.py::TestParametrized::test_various_block_sizes[10000]
            location: ('tests/test_parametrized.py', 41, 'TestParametrized.test_various_block_sizes[10000]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_parallel_faster_than_sequential>
          nextitem: <Function test_memory_efficiency_streaming>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential
            location: ('tests/test_performance_and_concurrency.py', 10, 'TestPerformance.test_parallel_faster_than_sequential')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_parallel_faster_than_sequential>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_faster_than_sequential>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_parallel_faster_than_sequential>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x1079710d0> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_faster_than_sequential>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_parallel_faster_than_sequential>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_parallel_faster_than_sequential>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_faster_than_sequential>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_parallel_faster_than_sequential>
            nextitem: <Function test_memory_efficiency_streaming>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_parallel_faster_than_sequential>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_parallel_faster_than_sequential>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_parallel_faster_than_sequential>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_parallel_faster_than_sequential
            location: ('tests/test_performance_and_concurrency.py', 10, 'TestPerformance.test_parallel_faster_than_sequential')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_memory_efficiency_streaming>
          nextitem: <Function test_worker_run_chunk>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming
            location: ('tests/test_performance_and_concurrency.py', 30, 'TestPerformance.test_memory_efficiency_streaming')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_memory_efficiency_streaming>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_memory_efficiency_streaming>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_memory_efficiency_streaming>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107969850> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_memory_efficiency_streaming>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_memory_efficiency_streaming>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_memory_efficiency_streaming>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_memory_efficiency_streaming>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_memory_efficiency_streaming>
            nextitem: <Function test_worker_run_chunk>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_memory_efficiency_streaming>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_memory_efficiency_streaming>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_memory_efficiency_streaming>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_memory_efficiency_streaming
            location: ('tests/test_performance_and_concurrency.py', 30, 'TestPerformance.test_memory_efficiency_streaming')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_worker_run_chunk>
          nextitem: <Function test_seed_reproducibility_across_runs>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk
            location: ('tests/test_performance_and_concurrency.py', 41, 'TestPerformance.test_worker_run_chunk')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_worker_run_chunk>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_worker_run_chunk>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_worker_run_chunk>>
          finish pytest_fixture_setup --> <conftest.SimpleSim object at 0x107969d60> [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_worker_run_chunk>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_worker_run_chunk>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_worker_run_chunk>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_worker_run_chunk>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_worker_run_chunk>
            nextitem: <Function test_seed_reproducibility_across_runs>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='simple_simulation' scope='function' baseid='tests'>
              request: <SubRequest 'simple_simulation' for <Function test_worker_run_chunk>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_worker_run_chunk>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_worker_run_chunk>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_performance_and_concurrency.py::TestPerformance::test_worker_run_chunk
            location: ('tests/test_performance_and_concurrency.py', 41, 'TestPerformance.test_worker_run_chunk')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_seed_reproducibility_across_runs>
          nextitem: <Function test_stats_engine_percentile_merge>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs
            location: ('tests/test_regression.py', 9, 'TestRegression.test_seed_reproducibility_across_runs')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_seed_reproducibility_across_runs>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_seed_reproducibility_across_runs>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_seed_reproducibility_across_runs>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_seed_reproducibility_across_runs>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_seed_reproducibility_across_runs>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_seed_reproducibility_across_runs>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_seed_reproducibility_across_runs>
            nextitem: <Function test_stats_engine_percentile_merge>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_seed_reproducibility_across_runs>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_seed_reproducibility_across_runs>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_regression.py::TestRegression::test_seed_reproducibility_across_runs
            location: ('tests/test_regression.py', 9, 'TestRegression.test_seed_reproducibility_across_runs')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_stats_engine_percentile_merge>
          nextitem: <Function test_portfolio_gbm_vs_non_gbm_consistency>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge
            location: ('tests/test_regression.py', 21, 'TestRegression.test_stats_engine_percentile_merge')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_stats_engine_percentile_merge>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_percentile_merge>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentile_merge>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_stats_engine_percentile_merge>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_stats_engine_percentile_merge>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentile_merge>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_stats_engine_percentile_merge>
            nextitem: <Function test_portfolio_gbm_vs_non_gbm_consistency>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_stats_engine_percentile_merge>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_stats_engine_percentile_merge>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_regression.py::TestRegression::test_stats_engine_percentile_merge
            location: ('tests/test_regression.py', 21, 'TestRegression.test_stats_engine_percentile_merge')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
          nextitem: <Function test_pi_estimation_initialization>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency
            location: ('tests/test_regression.py', 39, 'TestRegression.test_portfolio_gbm_vs_non_gbm_consistency')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_gbm_vs_non_gbm_consistency>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_gbm_vs_non_gbm_consistency>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
            nextitem: <Function test_pi_estimation_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_gbm_vs_non_gbm_consistency>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_gbm_vs_non_gbm_consistency>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_regression.py::TestRegression::test_portfolio_gbm_vs_non_gbm_consistency
            location: ('tests/test_regression.py', 39, 'TestRegression.test_portfolio_gbm_vs_non_gbm_consistency')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_pi_estimation_initialization>
          nextitem: <Function test_pi_estimation_single_run>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization
            location: ('tests/test_sims.py', 9, 'TestPiEstimationSimulation.test_pi_estimation_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_pi_estimation_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_initialization>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_initialization>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_pi_estimation_initialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_pi_estimation_initialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_initialization>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_pi_estimation_initialization>
            nextitem: <Function test_pi_estimation_single_run>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_initialization>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_initialization
            location: ('tests/test_sims.py', 9, 'TestPiEstimationSimulation.test_pi_estimation_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_pi_estimation_single_run>
          nextitem: <Function test_pi_estimation_convergence>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run
            location: ('tests/test_sims.py', 14, 'TestPiEstimationSimulation.test_pi_estimation_single_run')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_pi_estimation_single_run>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_single_run>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_single_run>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_pi_estimation_single_run>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_pi_estimation_single_run>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_single_run>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_pi_estimation_single_run>
            nextitem: <Function test_pi_estimation_convergence>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_single_run>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_single_run>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_single_run
            location: ('tests/test_sims.py', 14, 'TestPiEstimationSimulation.test_pi_estimation_single_run')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_pi_estimation_convergence>
          nextitem: <Function test_pi_estimation_antithetic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence
            location: ('tests/test_sims.py', 21, 'TestPiEstimationSimulation.test_pi_estimation_convergence')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_pi_estimation_convergence>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_convergence>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_convergence>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_pi_estimation_convergence>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_pi_estimation_convergence>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_convergence>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_pi_estimation_convergence>
            nextitem: <Function test_pi_estimation_antithetic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_convergence>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_convergence>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_convergence
            location: ('tests/test_sims.py', 21, 'TestPiEstimationSimulation.test_pi_estimation_convergence')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_pi_estimation_antithetic>
          nextitem: <Function test_pi_estimation_full_run>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic
            location: ('tests/test_sims.py', 35, 'TestPiEstimationSimulation.test_pi_estimation_antithetic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_pi_estimation_antithetic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_antithetic>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_antithetic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_pi_estimation_antithetic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_pi_estimation_antithetic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_antithetic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_pi_estimation_antithetic>
            nextitem: <Function test_pi_estimation_full_run>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_antithetic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_antithetic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_antithetic
            location: ('tests/test_sims.py', 35, 'TestPiEstimationSimulation.test_pi_estimation_antithetic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_pi_estimation_full_run>
          nextitem: <Function test_portfolio_initialization>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run
            location: ('tests/test_sims.py', 42, 'TestPiEstimationSimulation.test_pi_estimation_full_run')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_pi_estimation_full_run>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_full_run>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_full_run>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_pi_estimation_full_run>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_pi_estimation_full_run>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_full_run>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_pi_estimation_full_run>
            nextitem: <Function test_portfolio_initialization>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_pi_estimation_full_run>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_pi_estimation_full_run>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPiEstimationSimulation::test_pi_estimation_full_run
            location: ('tests/test_sims.py', 42, 'TestPiEstimationSimulation.test_pi_estimation_full_run')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_initialization>
          nextitem: <Function test_portfolio_single_run_basic>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization
            location: ('tests/test_sims.py', 55, 'TestPortfolioSimulation.test_portfolio_initialization')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_initialization>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_initialization>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_initialization>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_initialization>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_initialization>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_initialization>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_initialization>
            nextitem: <Function test_portfolio_single_run_basic>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_initialization>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_initialization>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_initialization
            location: ('tests/test_sims.py', 55, 'TestPortfolioSimulation.test_portfolio_initialization')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_single_run_basic>
          nextitem: <Function test_portfolio_positive_return_on_average>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic
            location: ('tests/test_sims.py', 60, 'TestPortfolioSimulation.test_portfolio_single_run_basic')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_single_run_basic>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_single_run_basic>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_single_run_basic>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_single_run_basic>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_single_run_basic>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_single_run_basic>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_single_run_basic>
            nextitem: <Function test_portfolio_positive_return_on_average>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_single_run_basic>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_single_run_basic>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_single_run_basic
            location: ('tests/test_sims.py', 60, 'TestPortfolioSimulation.test_portfolio_single_run_basic')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_positive_return_on_average>
          nextitem: <Function test_portfolio_non_gbm>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average
            location: ('tests/test_sims.py', 72, 'TestPortfolioSimulation.test_portfolio_positive_return_on_average')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_positive_return_on_average>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_positive_return_on_average>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_positive_return_on_average>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_positive_return_on_average>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_positive_return_on_average>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_positive_return_on_average>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_positive_return_on_average>
            nextitem: <Function test_portfolio_non_gbm>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_positive_return_on_average>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_positive_return_on_average>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_positive_return_on_average
            location: ('tests/test_sims.py', 72, 'TestPortfolioSimulation.test_portfolio_positive_return_on_average')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_non_gbm>
          nextitem: <Function test_portfolio_zero_volatility>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm
            location: ('tests/test_sims.py', 88, 'TestPortfolioSimulation.test_portfolio_non_gbm')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_non_gbm>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_non_gbm>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_non_gbm>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_non_gbm>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_non_gbm>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_non_gbm>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_non_gbm>
            nextitem: <Function test_portfolio_zero_volatility>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_non_gbm>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_non_gbm>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_non_gbm
            location: ('tests/test_sims.py', 88, 'TestPortfolioSimulation.test_portfolio_non_gbm')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_portfolio_zero_volatility>
          nextitem: <Function test_engine_creation>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility
            location: ('tests/test_sims.py', 101, 'TestPortfolioSimulation.test_portfolio_zero_volatility')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_portfolio_zero_volatility>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_zero_volatility>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_zero_volatility>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_portfolio_zero_volatility>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_portfolio_zero_volatility>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_zero_volatility>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_portfolio_zero_volatility>
            nextitem: <Function test_engine_creation>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_portfolio_zero_volatility>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_portfolio_zero_volatility>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_sims.py::TestPortfolioSimulation::test_portfolio_zero_volatility
            location: ('tests/test_sims.py', 101, 'TestPortfolioSimulation.test_portfolio_zero_volatility')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_engine_creation>
          nextitem: <Function test_engine_compute>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_creation
            location: ('tests/test_stats_engine.py', 8, 'TestStatsEngine.test_engine_creation')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_engine_creation>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_creation>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_creation>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_engine_creation>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_engine_creation>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_creation>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_engine_creation>
            nextitem: <Function test_engine_compute>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_creation>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_creation>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_creation' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_creation
            location: ('tests/test_stats_engine.py', 8, 'TestStatsEngine.test_engine_creation')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_engine_compute>
          nextitem: <Function test_default_engine_build>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_compute
            location: ('tests/test_stats_engine.py', 17, 'TestStatsEngine.test_engine_compute')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_engine_compute>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_compute>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_engine_compute>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_engine_compute>>
          finish pytest_fixture_setup --> [hook]
              n: 1000
              confidence: 0.95
              nan_policy: propagate
              ci_method: auto
              percentiles: (5, 25, 50, 75, 95)
              target: 0.0
              eps: 0.5
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_compute>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_engine_compute>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_engine_compute>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_compute>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_engine_compute>
            nextitem: <Function test_default_engine_build>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_compute>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_compute' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_compute
            location: ('tests/test_stats_engine.py', 17, 'TestStatsEngine.test_engine_compute')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_default_engine_build>
          nextitem: <Function test_default_engine_compute>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build
            location: ('tests/test_stats_engine.py', 28, 'TestStatsEngine.test_default_engine_build')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_default_engine_build>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_engine_build>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_build>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_default_engine_build>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_default_engine_build>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_build>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_default_engine_build>
            nextitem: <Function test_default_engine_compute>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_engine_build>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_build>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_default_engine_build
            location: ('tests/test_stats_engine.py', 28, 'TestStatsEngine.test_default_engine_build')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_default_engine_compute>
          nextitem: <Function test_engine_without_dist_free>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute
            location: ('tests/test_stats_engine.py', 34, 'TestStatsEngine.test_default_engine_compute')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_default_engine_compute>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_engine_compute>>
          finish pytest_fixture_setup --> None [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_default_engine_compute>>
          finish pytest_fixture_setup --> [ 5.99342831  4.7234714   6.29537708  8.04605971  4.53169325  4.53172609
  8.15842563  6.53486946  4.06105123  6.08512009  4.07316461  4.06854049
  5.48392454  1.17343951  1.55016433  3.87542494  2.97433776  5.62849467
  3.18395185  2.1753926   7.93129754  4.5484474   5.13505641  2.15050363
  3.91123455  5.22184518  2.69801285  5.75139604  3.79872262  4.4166125
  3.79658678  8.70455637  4.97300555  2.88457814  6.64508982  2.5583127
  5.41772719  1.08065975  2.3436279   5.39372247  6.47693316  5.34273656
  4.76870344  4.39779261  2.04295602  3.56031158  4.07872246  7.11424445
  5.68723658  1.47391969  5.64816794  4.22983544  3.646156    6.22335258
  7.06199904  6.86256024  3.32156495  4.38157525  5.66252686  6.95109025
  4.04165152  4.62868205  2.78733005  2.60758675  6.62505164  7.71248006
  4.85597976  7.0070658   5.72327205  3.70976049  5.72279121  8.07607313
  4.92834792  8.12928731 -0.23949021  6.64380501  5.17409414  4.4019853
  5.18352155  1.02486217  4.56065622  5.71422514  7.95578809  3.96345956
  3.38301279  3.99648591  6.83080424  5.65750222  3.94047959  6.02653487
  5.1941551   6.93728998  3.59589381  4.34467571  4.21578369  2.0729701
  5.59224055  5.52211054  5.01022691  4.53082573  2.16925852  4.15870935
  4.31457097  3.39544546  4.67742858  5.80810171  8.7723718   5.34915563
  5.51510078  4.85110817  1.16245757  4.94697225  5.12046042  9.92648422
  4.61527807  5.60309468  4.93057646  2.66264392  7.28564563  6.50386607
  6.58206389  3.18122509  7.80558862  2.19629787  6.17371419  9.38091125
  3.01892735  3.86740454  5.19930273  3.99304869  1.89867314  5.13712595
  2.87539257  5.94718486  3.16115153  8.09986881  3.43349342  4.35587697
  6.62703443  2.53827137  5.45491987  7.61428551  1.78503353  5.36926772
  5.51976559  6.56364574  2.52609858  2.35908677  6.04388313  5.59396935
  5.5009857   5.69289642  3.63995056  5.46450739  5.58614495  3.57129716
  8.73154902  5.94766584  2.61739301  6.31310722  3.05063666  6.57416921
  7.31719116  3.35863536  6.92675226  5.82556185  6.64412032  8.79358597
  4.50922377  3.49252767  3.22097114  3.36837943  4.84579658  5.68230395
  5.5533816   6.6543665   5.02600378  7.90706815  4.47068633 10.44033833
  6.2513347   3.28568489  2.858215    5.96494483  4.55307443  6.42800099
  5.94647525  4.85434217  3.30641256  1.97030555  4.1069701   6.71279759
  5.42818749  2.50852244  5.34636185  5.77063476  3.23228513  5.30745021
  5.11641744  2.7140594   5.71557472  6.12156905  7.16610249  7.1076041
  2.24466126  3.12434992  6.03007053  6.0275719   6.03009537 12.70546298
  6.14178102  7.27113128  6.90800353  6.3027825   4.36946151  6.51793844
  3.45434957  4.52636279  4.0292729   5.16374828  9.62931713  1.26546961
  6.37252038  1.77456826  4.05613627  7.17790119  5.12856004  2.84451044
  3.56939258  6.3591955   3.53926674  5.43291718  5.09114368  3.6967993
  9.28788818  6.26783804  0.94971483  5.37290863  3.67642707  6.70486667
  3.41495852  4.77052712  6.00997456  6.73151039  2.59940719  4.33099753
  4.05010938  3.69334153  8.53090848  5.80996342  2.47823209  6.83572389
  9.24431239  7.06493052  1.96126007  4.03153185  7.5338223   3.58466107
  5.88763886  6.54926811  3.14613906  4.88094929 -1.48253468  2.95122472
  4.4948637   2.50443364  8.26482261  2.13971724  4.11991103  5.26148115
  7.88254658  2.1282757   7.3263275   5.02046612  3.0369827   5.92420695
  5.39811939  3.79956625  5.13960417  4.22937281  5.22703469  6.32426135
  8.17203363  2.524369    9.26606675  1.0958244   4.69642981  6.17663441
  5.56198374  3.75460096  4.5837555   4.01399813  3.82127049  6.69920419
  5.71403097  3.61418081  6.79919975  5.61459904  6.62572424  6.25925768
  3.34200998  3.87963792  6.49458721  6.22074053  4.95819681  5.23465477
  7.55532979  3.81685722  6.09419476  4.5956147   4.56463759  7.1975537
  6.6508327   6.62701927  7.61095761  5.04200768  6.36390594  4.37946649
  5.6483327   4.73971389  5.19399193  6.19031405  3.36355863  9.18477455
  2.98796524  2.57162277  7.31622175  6.58332539  6.24823963  6.25669102
  4.97550645  3.20549126  5.15160912  3.64567658  6.95023947  4.70588524
  3.34900561  4.35722832  5.82586291  3.87255089  3.35555921  5.48737442
  5.48993314  3.98611365  4.05792339  5.46409987  2.10383132  2.18507245
  3.56311156  4.5731057   5.62181513  7.95071243  6.71531925  4.68012294
  4.96196758  2.99494127  4.96297373  4.42268272  5.64543712  3.34553811
  6.03869303  8.06547783  4.7824797   5.80342344  6.38028798  4.19755906
  5.44818496  5.0251848   5.1953522   3.45398043  5.04902035  5.99599658
  7.90228722  6.91854165  9.30636492  3.46530487  6.74464127  5.36668401
  9.37960587  3.38340343  3.32055632  3.80121471  0.75220855  3.94848996
  3.48173468  5.30078757  5.68351195  8.75234168  6.90084768  3.84619269
  3.20317066  5.98383834  2.35953359  8.66291753  7.35888024  4.0616487
  1.57373094  7.70774475  4.77092031  7.47563262  1.81114468  3.80124995
  5.0104874   5.09396119  4.09986906  6.24569986  2.86475914  4.71524103
  5.24059126  6.02887767  6.42322976  2.75071582  1.93177166  7.55535364
  5.66462802  3.50302693  8.10230395  5.23134927  7.35859437  5.13503696
  9.12149585  8.51068168  4.5020717   6.9431419   6.2907519   7.73726312
  3.07015308  6.37210292  7.11684897  1.48252103  2.63348297  0.92153564
  4.46118633  6.43508451  8.0047141   5.14818956  8.25723109  2.23979708
  1.59323512  4.8889046   5.7681309   4.9346105   0.8651158   4.82175992
  2.391061    6.3393451   5.73319649  3.12024043  3.97226617  2.88157296
  4.87464181  6.91028464  3.02854791  6.00809303  3.93948476  3.41425434
  4.78593928  2.92951536  3.89270139  2.60424421  8.92945027  5.0705271
  3.60054898  5.42795982  4.7753439   4.5580608   6.2283334   6.51501542
  3.9389977   3.84836352  4.44989661  0.39615767  1.96961788  7.73374853
  8.28993543  4.50192792  6.15311393  5.62250031 11.15776162  7.23914982
  4.74416482  3.08891912  1.78710736  5.40692727  3.48729851  2.15549258
  3.70685423  2.83690399  8.37428327  6.76327951  4.98405472  7.95988828
  5.15473662  3.2774316   8.04624815  6.07782009  2.92550769  4.61932264
  3.24876349  2.23440054  6.8523551   8.81883328  2.20286485  6.12593847
  3.69871486  4.02574923  3.81521215  3.27201846  5.09704326  3.33809977
  5.54091365  4.89952378  4.52210391  3.18487268  3.84645734  6.51078245
  6.00183438  3.04488951  5.19866461  6.50277425  1.66118944  6.08672038
  3.67475248  6.14119734  3.47348169  1.3902358   1.74491512  5.09616989
  5.519445    3.19136675  6.27718492  1.67695988  4.8678404   2.5779676
  3.69632778  5.09479734  3.27917327  4.23088891  7.01258562  3.84621626
  6.67138422  2.74058629  6.05960836  7.88313724  0.056711    3.40620949
  6.15414425  4.59390923  5.74229175  3.79202963  5.17317957  4.68864553
  7.33556412  5.50884169  5.67520532  4.17624607  4.02478755  4.13488362
  5.78890428  4.15803104  5.57954971  9.1508016   6.74224941  4.34795294
  7.40242784  4.18384925  0.92375093  2.98382738  1.25841616  4.29697303
  5.03683676  8.35287462  5.65385475  4.56179894  6.65881116  0.57772938
  5.47122912  6.54173039  2.04282751  7.28750809  5.67699281  4.16942417
  6.26556373  9.54138572  5.36373251  5.49644117  4.0812782   3.30031126
  6.66067163  3.28783235  5.14313247  4.04468511  5.95795965  5.66732421
  7.07507989  3.9799672   4.46025013  3.04247257  4.11141348  5.75460099
  6.51397723  3.15566935  6.73921184  7.71127572  5.82686981  8.75359163
  3.4524216   2.51069059  1.4425595   7.99208862  6.30873131  4.88883066
  5.55993725  2.74902191  9.89150396  5.25844236  5.21878959  6.45153325
  5.96201846  5.44776805  3.41905109  5.94293671  8.76404899  7.69084009
  8.18637325  3.97756865  3.02079036  4.74842616  5.11144982  7.18838304
  1.61507074  8.05910064  4.6839842   4.14623786  2.97579125  1.69028666
  6.64634117  5.14663593  2.4200782   2.40984246  4.3284306   8.33804305
  4.4808173   1.99371409  4.50851387  4.45455286 -0.39377329  4.89141027
  4.53813094  6.39241273  8.69791219  7.25313006  4.46222262  2.78694818
 10.14671961  5.11843687  5.02785858  4.95174983  5.39616952  4.71127918
  3.85267599  3.90628212  4.93449346  3.91315046  3.57430843  5.21286046
  4.49004557  8.00798598 -0.30193962  7.1830137   7.49217038  0.85321954
  4.31462481  4.25711827  2.18497661  3.44436662  2.77884831  8.50454089
  6.87135679  7.54311019  6.44334413  2.74189646  3.95095947  5.97874912
  2.55574438  6.42599686  4.5193492   4.25035838  6.42191994  5.88852662
  4.27806767  7.31865961  2.83787334  6.23187121  6.18620252  4.38090712
  5.65226604  2.49777285  6.84805404  4.63019573  3.95455396  7.09801845
  3.59131262  2.18307741  1.88674165  6.2120199   2.4391413   8.50958836
  0.83614118  8.39291274  5.42203493  4.80657378  3.91016183  5.79827223
  4.9247306   7.20660376  5.2284553   5.30060352  4.27277558  4.88610875
  5.61560354  1.57966321  2.30362916  6.48652819  5.34173088  4.63203333
  5.03686787  5.69516341  3.92048064  3.44339055  5.39169051  3.04325444
  5.81650551  1.59483279  7.05831127  5.94519496  5.51205947  6.96538197
  8.33094889  7.02874013  1.31825154  2.44084607  3.75036284  5.0521821
  6.03531804  3.54851237  5.37353353  3.48923414  3.77696439  2.18667781
  3.15353351  2.29663079  3.04825349  7.10728359  3.10120222 10.26476413
  5.9866358   5.36967225  3.28328444  6.40061976  3.84872435  5.24401963
 10.12016908  4.8078802   7.29854665  3.59364715  4.93002302  8.54160127
  3.74606588  8.62489712  6.41550387  3.87506645  6.26481548  6.9451089
  6.24361992  1.85955056  3.54572565  4.50496273  4.85113314  6.2413442
  5.355402    2.32931128  5.7603957   6.22117149  6.1195809   7.16156145
  6.66784431  5.91836016  4.85966858  1.67807813  5.85923644  5.41537537
  5.54315767  2.44650285  2.83788692  7.10630571  4.92088969  6.36300139
  5.05663675  5.05951228  6.87656761  3.96791054  5.19224155  4.07544942
  4.13100755  4.38165575  5.44426754  4.04250276  7.51151225  3.2107854
  4.62625671  4.12053788  7.89395577  5.39310955  7.06368908  2.02887925
  5.53410053  6.77926159  5.16456798  7.13096075  3.9654231   7.81869488
  9.59779625  4.27432288  4.10899496  7.90676895  8.15914429  3.95427995
  4.15962637  4.43643078  2.31109898  3.16269611  2.99171847  3.46440487
  4.93063023  5.46842947  8.10100099  3.00329192  6.9686448   4.57202231
  4.90107258  6.34963898  2.75455596  5.76481949  5.33290442  5.98490253
  5.57833729  9.91060028  3.72452003  3.93800609  3.75371895  3.88904576
  3.72522575  7.37803306  7.8410085   3.85850741  3.33528885  5.94283111
  3.89555391  6.26586364  5.40584604  1.96851177  8.0950104   8.59175535
  3.77442262  4.22459688  5.57173078  5.66891358  6.31708855  9.02040908
  4.64610555  3.40340551  2.24136154  3.53813992  4.93374605  8.58911573
  3.9647774   5.4475759   4.96715421  7.37678655 10.05386485  3.93826245
  4.02112111  7.08832175  6.36378298  8.69341465  6.16785637  4.28141582
  6.18130966  7.21740716  6.64096436  6.01454806  7.13334938  7.33859118
  7.76431798  6.29741978  4.66576384  5.29342737  7.41301793  3.36612866
  5.73734662  4.21332238  5.05748965  7.55690373  5.38219814  5.0928731
  2.28028772  6.49250713  6.29096836  9.32650945  4.38444353  5.43830066
  5.49876737  8.15490656  4.80940894  5.55804305  6.21579302  5.37321825
  4.10713277  5.38817999  7.1472635   2.9469694   5.26593935  3.59975837
  7.39009326  1.95362619  3.88215631  5.75442375  8.13104806  4.86849948
  3.88960095  8.76231414  2.1039722   0.60238809  5.8800289   3.99589155
  2.95753437  6.41671289  5.48760143  3.87184274  2.4393912   6.74491466
  6.30040236  4.80164827  8.69327399  2.85983047  1.94894966  3.61618386
  4.90882797  5.4866789   4.51752788  5.70411079  2.49692115  7.88752921
  4.83569764  7.23459166  5.68545069  5.91350644  6.13953456  5.89541712
  6.28544552  7.65830506  5.39304234  6.41800752  4.82052861  7.88023443
  3.6472154   8.60188087  4.9196841   2.1384498   5.25620883  3.63789669
  6.6812871   3.69475204  4.10763313  1.22091854  4.09538736  0.15224135
  1.83219435  6.52082931  6.57160032  5.85091512  3.06604771  4.90457729
  4.99279492  2.68327062  8.0067966   6.75472458  4.55807165  5.05377168
  5.41676562  0.91653026  4.50564523  3.6360315   2.99675998  4.43779941
  8.59537305  6.28168572  3.85764202  6.14516556] [hook]
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_default_engine_compute>>
          finish pytest_fixture_setup --> [hook]
              n: 1000
              confidence: 0.95
              nan_policy: propagate
              ci_method: auto
              percentiles: (5, 25, 50, 75, 95)
              target: 0.0
              eps: 0.5
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_compute>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_default_engine_compute>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_default_engine_compute>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_compute>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_default_engine_compute>
            nextitem: <Function test_engine_without_dist_free>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='ctx_basic' scope='function' baseid='tests'>
              request: <SubRequest 'ctx_basic' for <Function test_default_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='sample_data' scope='function' baseid='tests'>
              request: <SubRequest 'sample_data' for <Function test_default_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_default_engine_compute>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_default_engine_compute>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_default_engine_compute
            location: ('tests/test_stats_engine.py', 34, 'TestStatsEngine.test_default_engine_compute')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_engine_without_dist_free>
          nextitem: <Function test_engine_without_target_bounds>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free
            location: ('tests/test_stats_engine.py', 43, 'TestStatsEngine.test_engine_without_dist_free')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_engine_without_dist_free>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_without_dist_free>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_dist_free>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_engine_without_dist_free>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_engine_without_dist_free>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_dist_free>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_engine_without_dist_free>
            nextitem: <Function test_engine_without_target_bounds>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_without_dist_free>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_dist_free>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_without_dist_free
            location: ('tests/test_stats_engine.py', 43, 'TestStatsEngine.test_engine_without_dist_free')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_engine_without_target_bounds>
          nextitem: <Function test_z_crit_95_confidence>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds
            location: ('tests/test_stats_engine.py', 49, 'TestStatsEngine.test_engine_without_target_bounds')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_engine_without_target_bounds>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_without_target_bounds>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_target_bounds>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_engine_without_target_bounds>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_engine_without_target_bounds>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_target_bounds>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_engine_without_target_bounds>
            nextitem: <Function test_z_crit_95_confidence>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_engine_without_target_bounds>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_engine_without_target_bounds>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_stats_engine.py::TestStatsEngine::test_engine_without_target_bounds
            location: ('tests/test_stats_engine.py', 49, 'TestStatsEngine.test_engine_without_target_bounds')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_z_crit_95_confidence>
          nextitem: <Function test_z_crit_99_confidence>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence
            location: ('tests/test_utils.py', 8, 'TestCriticalValues.test_z_crit_95_confidence')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_z_crit_95_confidence>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_95_confidence>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_95_confidence>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_z_crit_95_confidence>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_z_crit_95_confidence>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_95_confidence>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_z_crit_95_confidence>
            nextitem: <Function test_z_crit_99_confidence>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_95_confidence>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_95_confidence>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_95_confidence
            location: ('tests/test_utils.py', 8, 'TestCriticalValues.test_z_crit_95_confidence')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_z_crit_99_confidence>
          nextitem: <Function test_z_crit_invalid_confidence_too_low>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence
            location: ('tests/test_utils.py', 13, 'TestCriticalValues.test_z_crit_99_confidence')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_z_crit_99_confidence>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_99_confidence>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_99_confidence>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_z_crit_99_confidence>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_z_crit_99_confidence>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_99_confidence>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_z_crit_99_confidence>
            nextitem: <Function test_z_crit_invalid_confidence_too_low>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_99_confidence>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_99_confidence>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_99_confidence
            location: ('tests/test_utils.py', 13, 'TestCriticalValues.test_z_crit_99_confidence')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_z_crit_invalid_confidence_too_low>
          nextitem: <Function test_z_crit_invalid_confidence_too_high>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low
            location: ('tests/test_utils.py', 18, 'TestCriticalValues.test_z_crit_invalid_confidence_too_low')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_invalid_confidence_too_low>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_z_crit_invalid_confidence_too_low>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
            nextitem: <Function test_z_crit_invalid_confidence_too_high>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_invalid_confidence_too_low>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_low>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_low
            location: ('tests/test_utils.py', 18, 'TestCriticalValues.test_z_crit_invalid_confidence_too_low')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_z_crit_invalid_confidence_too_high>
          nextitem: <Function test_t_crit_95_confidence_30_df>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high
            location: ('tests/test_utils.py', 23, 'TestCriticalValues.test_z_crit_invalid_confidence_too_high')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_invalid_confidence_too_high>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_z_crit_invalid_confidence_too_high>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
            nextitem: <Function test_t_crit_95_confidence_30_df>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_z_crit_invalid_confidence_too_high>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_z_crit_invalid_confidence_too_high>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_z_crit_invalid_confidence_too_high
            location: ('tests/test_utils.py', 23, 'TestCriticalValues.test_z_crit_invalid_confidence_too_high')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_t_crit_95_confidence_30_df>
          nextitem: <Function test_t_crit_small_df>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df
            location: ('tests/test_utils.py', 28, 'TestCriticalValues.test_t_crit_95_confidence_30_df')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_t_crit_95_confidence_30_df>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_95_confidence_30_df>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_95_confidence_30_df>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_t_crit_95_confidence_30_df>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_t_crit_95_confidence_30_df>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_95_confidence_30_df>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_t_crit_95_confidence_30_df>
            nextitem: <Function test_t_crit_small_df>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_95_confidence_30_df>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_95_confidence_30_df>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_95_confidence_30_df
            location: ('tests/test_utils.py', 28, 'TestCriticalValues.test_t_crit_95_confidence_30_df')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_t_crit_small_df>
          nextitem: <Function test_t_crit_invalid_df>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_small_df
            location: ('tests/test_utils.py', 34, 'TestCriticalValues.test_t_crit_small_df')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_t_crit_small_df>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_small_df>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_small_df>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_t_crit_small_df>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_t_crit_small_df>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_small_df>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_t_crit_small_df>
            nextitem: <Function test_t_crit_invalid_df>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_small_df>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_small_df>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_small_df' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_small_df
            location: ('tests/test_utils.py', 34, 'TestCriticalValues.test_t_crit_small_df')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_t_crit_invalid_df>
          nextitem: <Function test_autocrit_large_n_uses_z>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df
            location: ('tests/test_utils.py', 39, 'TestCriticalValues.test_t_crit_invalid_df')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_t_crit_invalid_df>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_invalid_df>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_invalid_df>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_t_crit_invalid_df>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_t_crit_invalid_df>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_invalid_df>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_t_crit_invalid_df>
            nextitem: <Function test_autocrit_large_n_uses_z>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_t_crit_invalid_df>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_t_crit_invalid_df>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_t_crit_invalid_df
            location: ('tests/test_utils.py', 39, 'TestCriticalValues.test_t_crit_invalid_df')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_autocrit_large_n_uses_z>
          nextitem: <Function test_autocrit_small_n_uses_t>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z
            location: ('tests/test_utils.py', 44, 'TestCriticalValues.test_autocrit_large_n_uses_z')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_autocrit_large_n_uses_z>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_large_n_uses_z>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_large_n_uses_z>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_autocrit_large_n_uses_z>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_autocrit_large_n_uses_z>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_large_n_uses_z>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_autocrit_large_n_uses_z>
            nextitem: <Function test_autocrit_small_n_uses_t>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_large_n_uses_z>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_large_n_uses_z>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_large_n_uses_z
            location: ('tests/test_utils.py', 44, 'TestCriticalValues.test_autocrit_large_n_uses_z')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_autocrit_small_n_uses_t>
          nextitem: <Function test_autocrit_force_z>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t
            location: ('tests/test_utils.py', 50, 'TestCriticalValues.test_autocrit_small_n_uses_t')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_autocrit_small_n_uses_t>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_small_n_uses_t>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_small_n_uses_t>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_autocrit_small_n_uses_t>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_autocrit_small_n_uses_t>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_small_n_uses_t>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_autocrit_small_n_uses_t>
            nextitem: <Function test_autocrit_force_z>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_small_n_uses_t>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_small_n_uses_t>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_small_n_uses_t
            location: ('tests/test_utils.py', 50, 'TestCriticalValues.test_autocrit_small_n_uses_t')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_autocrit_force_z>
          nextitem: <Function test_autocrit_force_t>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_force_z
            location: ('tests/test_utils.py', 56, 'TestCriticalValues.test_autocrit_force_z')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_autocrit_force_z>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_force_z>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_z>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_autocrit_force_z>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_autocrit_force_z>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_z>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_autocrit_force_z>
            nextitem: <Function test_autocrit_force_t>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_force_z>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_z>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_z' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_force_z
            location: ('tests/test_utils.py', 56, 'TestCriticalValues.test_autocrit_force_z')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_autocrit_force_t>
          nextitem: <Function test_autocrit_invalid_method>
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_force_t
            location: ('tests/test_utils.py', 62, 'TestCriticalValues.test_autocrit_force_t')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_autocrit_force_t>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_force_t>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_t>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_autocrit_force_t>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_autocrit_force_t>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_t>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_autocrit_force_t>
            nextitem: <Function test_autocrit_invalid_method>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_force_t>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_force_t>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_force_t' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_force_t
            location: ('tests/test_utils.py', 62, 'TestCriticalValues.test_autocrit_force_t')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_autocrit_invalid_method>
          nextitem: None
        pytest_runtest_logstart [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method
            location: ('tests/test_utils.py', 68, 'TestCriticalValues.test_autocrit_invalid_method')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_autocrit_invalid_method>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_invalid_method>>
          finish pytest_fixture_setup --> None [hook]
        finish pytest_runtest_setup --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_invalid_method>
            call: <CallInfo when='setup' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_autocrit_invalid_method>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_autocrit_invalid_method>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_invalid_method>
            call: <CallInfo when='call' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_autocrit_invalid_method>
            nextitem: None
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_stable_seed' scope='function' baseid='tests'>
              request: <SubRequest '_stable_seed' for <Function test_autocrit_invalid_method>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='_set_spawn_start_method' scope='session' baseid='tests'>
              request: <SubRequest '_set_spawn_start_method' for <Function test_mean_simple>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> None [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_autocrit_invalid_method>
            call: <CallInfo when='teardown' result: None>
        finish pytest_runtest_makereport --> <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x100df67b0>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: tests/test_utils.py::TestCriticalValues::test_autocrit_invalid_method
            location: ('tests/test_utils.py', 68, 'TestCriticalValues.test_autocrit_invalid_method')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
    finish pytest_runtestloop --> True [hook]
    pytest_sessionfinish [hook]
        session: <Session  exitstatus=0 testsfailed=0 testscollected=142>
        exitstatus: 0
      pytest_terminal_summary [hook]
          terminalreporter: <_pytest.terminal.TerminalReporter object at 0x1073edeb0>
          exitstatus: 0
          config: <_pytest.config.Config object at 0x100df67b0>
      finish pytest_terminal_summary --> [] [hook]
    finish pytest_sessionfinish --> [] [hook]
    pytest_unconfigure [hook]
        config: <_pytest.config.Config object at 0x100df67b0>
    finish pytest_unconfigure --> [] [hook]
